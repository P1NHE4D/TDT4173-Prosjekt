{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77dc90fc-b581-4d9c-aec0-dfaee728d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d9d93-afa7-4dea-a014-34f3eb0cb3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea19ac-1770-43f6-8578-60cb3342e7c2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9fbc63a8-2e3a-4b00-9b8d-ef7abd01113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments_train = pd.read_csv('data/apartments_train.csv')\n",
    "buildings_train = pd.read_csv('data/buildings_train.csv')\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "\n",
    "train = pd.merge(\n",
    "    apartments_train, \n",
    "    buildings_train.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")\n",
    "test = pd.merge(\n",
    "    apartments_test, \n",
    "    buildings_test.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2bdce-fbf2-40da-a78a-c58a1be54e16",
   "metadata": {},
   "source": [
    "## Replace missing coordinates and districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d4dbd51-efc0-4c8d-94d5-c8ce941ea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = test[test[\"longitude\"].isna()].index\n",
    "test.at[ind, \"latitude\"] = 55.56776345324702\n",
    "test.at[ind, \"longitude\"] = 37.48171529826662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4376456d-cf66-4482-9f4a-47b925077a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train[train.district.isna()].index\n",
    "train.at[ind, \"district\"] = 12\n",
    "ind = test[test.district.isna()].index\n",
    "test.at[ind, \"district\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baac6a5-b58e-4fe1-8ad1-0a64f44722f9",
   "metadata": {},
   "source": [
    "## Fixing incorrect coordinates in testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8927b11e-f046-4548-b769-e53c70da927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartment_coordinates = {\n",
    "    2511: [55.5438629741104, 37.48233890768276],\n",
    "    2529: [55.66866253043727, 37.217152651528785],\n",
    "    4719: [55.63132797843279, 37.426744466015705],\n",
    "    5090: [55.54390546580924, 37.48229599441532],\n",
    "    6959: [55.54390546580924, 37.48229599441532],\n",
    "    8596: [55.54390546580924, 37.48229599441532],\n",
    "    9547: [55.63399775464791, 37.41982879208156]\n",
    "}\n",
    "for k, v in apartment_coordinates.items():\n",
    "    test.at[k, \"latitude\"] = v[0]\n",
    "    test.at[k, \"longitude\"] = v[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a5bac-acc9-411c-b834-7ed398a569c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf37425-8a12-4473-bf3b-15124149510f",
   "metadata": {},
   "source": [
    "## Target features in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "447d9a8d-bc1b-4528-bb01-06d844ce4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_price\"] = np.log1p(train.price)\n",
    "train[\"price_per_m2\"] = train.price / train.area_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921ffed-b1f6-4b94-a254-d53550abef4f",
   "metadata": {},
   "source": [
    "## Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ebf4a7c4-da99-4477-abf4-df3f95205cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_center_feature(data):\n",
    "    moscow_center = [55.751244, 37.618423]\n",
    "    coordinates = data[['latitude', 'longitude']].to_numpy()\n",
    "    dist = [geopy.distance.distance(moscow_center, coordinate).km for coordinate in coordinates]\n",
    "    data['distance_to_center'] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d6595f4c-371c-4470-8020-89579d8236cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_metro_feature(data, add_metro_lines=True):\n",
    "    d = data.copy()\n",
    "\n",
    "    metro_data = pd.read_csv(\"data/moscow_metro_data.csv\", delimiter=\";\")\n",
    "\n",
    "    # drop duplicates\n",
    "    ind = metro_data[metro_data[\"English transcription\"].duplicated()].index\n",
    "    metro_data = metro_data.drop(ind)\n",
    "    metro_data = metro_data.reset_index()\n",
    "    metro_lines = [\"metro_line_{}\".format(i) for i in range(1, 16)]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(metro_data[[\"latitude\", \"longitude\"]])\n",
    "    distances, indices = nbrs.kneighbors(d[[\"latitude\", \"longitude\"]])\n",
    "    d[\"metro_index\"] = indices\n",
    "\n",
    "    for ind, row in d.iterrows():\n",
    "        metro_coordinate = metro_data.loc[[row.metro_index]][[\"latitude\", \"longitude\"]].to_numpy()\n",
    "        distance = geopy.distance.distance([[row[\"latitude\"], row[\"longitude\"]]], metro_coordinate).km\n",
    "        data.at[ind, \"distance_to_metro\"] = distance\n",
    "\n",
    "        if add_metro_lines:\n",
    "            number_of_metro_lines = 0\n",
    "            metro_lines_data = metro_data.loc[[row.metro_index]][metro_lines]\n",
    "            for metro_line in metro_lines:\n",
    "                access_to_line = metro_lines_data[metro_line].values[0]\n",
    "                data.at[ind, metro_line] = access_to_line\n",
    "                if access_to_line == 1:\n",
    "                    number_of_metro_lines += 1\n",
    "            data.at[ind, \"number_of_metro_lines\"] = number_of_metro_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ccdd1d1-2503-4dfe-b274-6e94dc19a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearing_feature(data):\n",
    "    for ind, row in data.iterrows():\n",
    "        moscow_center = [55.751244, 37.618423]\n",
    "        c1 = moscow_center\n",
    "        c2 = [row[\"latitude\"], row[\"longitude\"]]\n",
    "        y = np.sin(c2[1] - c1[1]) * np.cos(c2[0])\n",
    "        x = np.cos(c1[0]) * np.sin(c2[0]) - np.sin(c1[0]) * np.cos(c2[0]) * np.cos(c2[1] - c1[1])\n",
    "        rad = np.arctan2(y, x)\n",
    "        bearing = ((rad * 180 / np.pi) + 360) % 360\n",
    "        data.at[ind, \"bearing\"] = bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d1ac44b-de0c-485b-a4cd-2c5f1880a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_hospital_feature(data):\n",
    "    d = data.copy()\n",
    "    hospital_data = pd.read_csv(\"data/hospitals.csv\")\n",
    "    hospital_data = hospital_data[[\"latitude\", \"longitude\"]]\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(hospital_data)\n",
    "    distances, indices = nbrs.kneighbors(data[[\"latitude\", \"longitude\"]])\n",
    "    d[\"hospital_index\"] = indices\n",
    "    distances = []\n",
    "    for ind, row in d.iterrows():\n",
    "        apartment_coordinates = [[row[\"latitude\"], row[\"longitude\"]]]\n",
    "        hospital_coordinate = hospital_data.loc[[row.hospital_index]].to_numpy()\n",
    "        distance = geopy.distance.distance(apartment_coordinates, hospital_coordinate).km\n",
    "        distances.append(distance)\n",
    "    data[\"distance_to_hospital\"] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dcfe680f-98ee-480f-ad3e-7d4abfa7a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_universities_feature(data):\n",
    "    state_uni_coordinates = [55.70444300116007, 37.528611852796914]\n",
    "    tech_uni_coordinates = [55.76666597872545, 37.68511242319504]\n",
    "    distances_to_state_uni = []\n",
    "    distances_to_tech_uni = []\n",
    "    for ind, row in data.iterrows():\n",
    "        distances_to_state_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], state_uni_coordinates).km)\n",
    "        distances_to_tech_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], tech_uni_coordinates).km)\n",
    "    data[\"distance_to_state_uni\"] = distances_to_state_uni\n",
    "    data[\"distance_to_tech_uni\"] = distances_to_tech_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d35c0b7b-313a-4d77-b3e3-be909eaabc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wealthy_districts_features(data):\n",
    "    with open(\"data/geojson/khamovniki_polygon_data.geojson\") as f:\n",
    "        gj_khamovniki = geojson.load(f)\n",
    "    with open(\"data/geojson/yakimanka_polygon_data.geojson\") as f:\n",
    "        gj_yakimanka = geojson.load(f)\n",
    "    with open(\"data/geojson/arbat_polygon_data.geojson\") as f:\n",
    "        gj_arbat = geojson.load(f)\n",
    "    with open(\"data/geojson/presnensky_polygon_data.geojson\") as f:\n",
    "        gj_presnensky = geojson.load(f)\n",
    "    with open(\"data/geojson/Tverskoy_polygon_data.geojson\") as f:\n",
    "        gj_tverskoy = geojson.load(f)\n",
    "    khamovniki_polygon = Polygon(gj_khamovniki[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    yakimanka_polygon = Polygon(gj_yakimanka[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    arbat_polygon = Polygon(gj_arbat[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    presnensky_polygon = Polygon(gj_presnensky[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    tverskoy_polygon = Polygon(gj_tverskoy[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    for ind, row in data.iterrows():\n",
    "        point = Point([row[\"longitude\"], row[\"latitude\"]])\n",
    "        data.at[ind, \"is_in_khamovniki\"] = 1 if khamovniki_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_yakimanka\"] = 1 if yakimanka_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_arbat\"] = 1 if arbat_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_presnensky\"] = 1 if presnensky_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_tverskoy\"] = 1 if tverskoy_polygon.contains(point) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e392b68d-0753-417b-8193-4a3272c498a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_box_areas_more_boxes(data):\n",
    "    \"\"\"only add values in list to prevent unnecessary slowdown\"\"\"\n",
    "    locs_with_high_corr = [\n",
    "            'loc_37.575_55.75', 'loc_37.6_55.725', 'loc_37.525_55.75',\n",
    "           'loc_37.55_55.775', 'loc_37.525_55.725', 'loc_37.575_55.725',\n",
    "           'loc_37.525_55.7', 'loc_37.5_55.7', 'loc_37.55_55.725',\n",
    "           'loc_37.475_55.7', 'loc_37.625_55.725', 'loc_37.5_55.75',\n",
    "           'loc_37.525_55.775', 'loc_37.525_55.675', 'loc_37.55_55.675',\n",
    "           'loc_37.55_55.75', 'loc_37.4_55.6', 'loc_37.725_55.575',\n",
    "           'loc_37.925_55.675', 'loc_37.475_55.55', 'loc_37.65_55.575',\n",
    "           'loc_37.525_55.875', 'loc_37.5_55.55', 'loc_37.475_55.525',\n",
    "           'loc_37.925_55.7', 'loc_37.6_55.75', 'loc_37.575_55.7', 'loc_37.55_55.7'\n",
    "    ]\n",
    "    for lon in np.arange(37.4, 37.95, 0.025):\n",
    "        for lat in np.arange(55.525, 55.9, 0.025):\n",
    "            column_name = \"loc_\" + str(round(lon, 3)) + \"_\" + str(round(lat, 3))\n",
    "            if column_name in locs_with_high_corr: #column_name in locs_with_high_corr:\n",
    "                data[column_name] = 0\n",
    "                indexes = data[\n",
    "                    (lon < data['longitude']) & (data['longitude'] < lon + 0.025) & (lat < data['latitude']) & (\n",
    "                                data['latitude'] < lat + 0.025)\n",
    "                    ].index\n",
    "\n",
    "                data.loc[indexes, column_name] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39934d84-67ee-45ab-aef4-30c8b37872c9",
   "metadata": {},
   "source": [
    "## Feature construction - (NOTE: TAKES SOME TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4e2cd22-1411-40a8-b927-db928bdc94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(data):\n",
    "    distance_to_center_feature(data)\n",
    "    distance_to_metro_feature(data)\n",
    "    bearing_feature(data)\n",
    "    distance_to_hospital_feature(data)\n",
    "    distance_to_universities_feature(data)\n",
    "    wealthy_districts_features(data)\n",
    "    add_box_areas_more_boxes(data)\n",
    "\n",
    "train_with_feats = train.copy()\n",
    "test_with_feats = test.copy()\n",
    "construct_features(train_with_feats)\n",
    "construct_features(test_with_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396131b3-6548-4874-996d-3e998e697caa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f158e7-3087-4006-aad6-bf3a80292cef",
   "metadata": {},
   "source": [
    "## Data imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "880db4a7-45f8-4b04-9ed2-1b81db98274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_bin_mean(data, train_data, test_data, feature, bin_feature, bins=40, decimals=0, verbose=False):\n",
    "    tr = train_data.copy()\n",
    "    te = test_data.copy()\n",
    "    database = pd.concat([tr, te])\n",
    "    database[\"bin\"], bins = pd.cut(database[bin_feature], bins=40, retbins=True)\n",
    "    pb = 0\n",
    "    for b in bins:\n",
    "        bin_data = database[(database[bin_feature] >= pb) & (database[bin_feature] <= b)]\n",
    "        bin_mean = bin_data[feature].mean()\n",
    "        percentage = (bin_mean / bin_data[bin_feature]).mean()\n",
    "        d = data[(data[bin_feature] >= pb) & (data[bin_feature] <= b)]\n",
    "        ind = d[d[feature].isna()].index\n",
    "        if pd.notna(bin_mean) and len(ind) > 0:\n",
    "            bin_mean = round(bin_mean, decimals)\n",
    "            data.at[ind, feature] = bin_mean\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Set {} for {} rows with {} <= area_living <= {} to {}\".format(feature, len(ind), pb, b, bin_mean))\n",
    "        pb = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6298b36f-48f9-45a8-8dd4-f448afb94a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_area_living_bins(train_data, test_data):\n",
    "    feature = \"area_living\"\n",
    "    impute_with_bin_mean(data=train_data, train_data=train_data, test_data=test_data, feature=feature, bin_feature=\"area_total\")\n",
    "    impute_with_bin_mean(data=test_data, train_data=train_data, test_data=test_data, feature=feature, bin_feature=\"area_total\")\n",
    "    d = train_data[train_data.area_living.isna()]\n",
    "    for ind, row in d.iterrows():\n",
    "        train_data.at[ind, feature] = 0.85 * row.area_total\n",
    "    d = test_data[test_data.area_living.isna()]\n",
    "    for ind, row in d.iterrows():\n",
    "        test_data.at[ind, feature] = 0.85 * row.area_total\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "def impute_area_kitchen_bins(train_data, test_data):\n",
    "    feature = \"area_kitchen\"\n",
    "    impute_with_bin_mean(data=train_data, train_data=train_data, test_data=test_data, feature=feature, bin_feature=\"area_total\")\n",
    "    impute_with_bin_mean(data=test_data, train_data=train_data, test_data=test_data, feature=feature, bin_feature=\"area_total\")\n",
    "    d = train_data[train_data.area_kitchen.isna()]\n",
    "    for ind, row in d.iterrows():\n",
    "        train_data.at[ind, feature] = 0.05 * row.area_total\n",
    "    d = test_data[test_data.area_kitchen.isna()]\n",
    "    for ind, row in d.iterrows():\n",
    "        test_data.at[ind, feature] = 0.05 * row.area_total\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def impute_constructed_bins(train_data, test_data):\n",
    "    feature = \"constructed\"\n",
    "    d = train_data[train_data.constructed.isna()]\n",
    "    ind = d[d.new == 1].index\n",
    "    train_data.at[ind, feature] = 2021\n",
    "    d = test_data[test_data.constructed.isna()]\n",
    "    ind = d[d.new == 1].index\n",
    "    test_data.at[ind, feature] = 2021\n",
    "    impute_with_bin_mean(data=train_data, train_data=train_data, test_data=test_data, feature=feature, bins=12, bin_feature=\"district\")\n",
    "    impute_with_bin_mean(data=test_data, train_data=train_data, test_data=test_data, feature=feature, bins=12, bin_feature=\"district\")\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "\n",
    "def impute_material_bins(train_data, test_data):\n",
    "    feature = \"material\"\n",
    "    impute_with_bin_mean(data=train_data, train_data=train_data, test_data=test_data, feature=feature, bins=20, bin_feature=\"constructed\")\n",
    "    impute_with_bin_mean(data=test_data, train_data=train_data, test_data=test_data, feature=feature, bins=20, bin_feature=\"constructed\")\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "\n",
    "def impute_heating_category(train_data, test_data):\n",
    "    feature = \"heating\"\n",
    "    ind = train_data[train_data.heating.isna()].index\n",
    "    train_data.at[ind, feature] = 4\n",
    "    ind = test_data[test_data.heating.isna()].index\n",
    "    test_data.at[ind, feature] = 4\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "\n",
    "def impute_parking_category(train_data, test_data):\n",
    "    feature = \"parking\"\n",
    "    ind = train_data[train_data.parking.isna()].index\n",
    "    train_data.at[ind, feature] = 3\n",
    "    ind = test_data[test_data.parking.isna()].index\n",
    "    test_data.at[ind, feature] = 3\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feature, len(train_data[train_data[feature].isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feature, len(test_data[test_data[feature].isna()])))\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def impute_remaining_nans(train_data, test_data):\n",
    "    train_data.loc[:, \"seller\"] = train_data.loc[:, \"seller\"].fillna(4)\n",
    "    test_data.loc[:, \"seller\"] = test_data.loc[:, \"seller\"].fillna(4)\n",
    "    train_data.loc[:, \"layout\"] = train_data.loc[:, \"layout\"].fillna(3)\n",
    "    test_data.loc[:, \"layout\"] = test_data.loc[:, \"layout\"].fillna(3)\n",
    "    train_data.loc[:, \"condition\"] = train_data.loc[:, \"condition\"].fillna(4)\n",
    "    test_data.loc[:, \"condition\"] = test_data.loc[:, \"condition\"].fillna(4)\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", \"seller\", len(train_data[train_data.seller.isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", \"seller\", len(test_data[test_data.seller.isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", \"layout\", len(train_data[train_data.layout.isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", \"layout\", len(test_data[test_data.layout.isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", \"condition\", len(train_data[train_data.condition.isna()])))\n",
    "    print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", \"condition\", len(test_data[test_data.condition.isna()])))\n",
    "    feats = [\"ceiling\", \"bathrooms_shared\", \"bathrooms_private\", \"windows_court\", \"windows_street\", \"garbage_chute\", \"balconies\", \"loggias\", \"phones\", \"elevator_passenger\", \"elevator_service\"]\n",
    "    for feat in feats:\n",
    "        impute_with_bin_mean(data=train_data, train_data=train_data, test_data=test_data, feature=feat, bin_feature=\"area_total\")\n",
    "        impute_with_bin_mean(data=test_data, train_data=train_data, test_data=test_data, feature=feat, bin_feature=\"area_total\")\n",
    "        fill_value = train_data[train_data[feat].notna()][feat].mean(axis=0).round()\n",
    "        train_data.loc[:, feat] = train_data.loc[:, feat].fillna(fill_value)\n",
    "        fill_value = test_data[test_data[feat].notna()][feat].mean(axis=0).round()\n",
    "        test_data.loc[:, feat] = test_data.loc[:, feat].fillna(fill_value)\n",
    "        print(\"Remaining nan values in {} for {}: {}\".format(\"train_data\", feat, len(train_data[train_data[feat].isna()])))\n",
    "        print(\"Remaining nan values in {} for {}: {}\".format(\"test_data\", feat, len(test_data[test_data[feat].isna()])))\n",
    "        \n",
    "    \n",
    "    train_data.loc[:, \"elevator_without\"] = train_data.loc[:, \"elevator_without\"].fillna(0)\n",
    "    test_data.loc[:, \"elevator_without\"] = test_data.loc[:, \"elevator_without\"].fillna(0)\n",
    "    train_data.loc[:, \"new\"] = train_data.loc[:, \"new\"].fillna(0)\n",
    "    test_data.loc[:, \"new\"] = test_data.loc[:, \"new\"].fillna(0)\n",
    "        \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "74d977ef-8c3e-4cc6-8b6a-02f257a3d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_data, test_data):\n",
    "    \n",
    "    ###### data imputation ######\n",
    "    print(\"Imputing missing data...\")\n",
    "    train_data, test_data = impute_area_living_bins(train_data, test_data)\n",
    "    train_data, test_data = impute_area_kitchen_bins(train_data, test_data)\n",
    "    train_data, test_data = impute_constructed_bins(train_data, test_data)\n",
    "    train_data, test_data = impute_material_bins(train_data, test_data)\n",
    "    train_data, test_data = impute_heating_category(train_data, test_data)\n",
    "    train_data, test_data = impute_parking_category(train_data, test_data)\n",
    "    train_data, test_data = impute_remaining_nans(train_data, test_data)\n",
    "\n",
    "    \n",
    "    return train_data, test_data\n",
    "    \n",
    "\n",
    "train_data = train_with_feats.copy()\n",
    "test_data = test_with_feats.copy()\n",
    "train_data, test_data = prepare_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a8d72-3269-4743-817d-f1b10b01027b",
   "metadata": {},
   "source": [
    "## One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "518f4b13-c4cf-47ac-b9ad-619b77375991",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_categories = {\n",
    "    3: \"East\",\n",
    "    6: \"South-West\",\n",
    "    5: \"South\",\n",
    "    4: \"South-East\",\n",
    "    0: \"Central\",\n",
    "    2: \"North-East\",\n",
    "    1: \"North\",\n",
    "    8: \"North-West\",\n",
    "    7: \"West\",\n",
    "    11: \"Novomoskovsk\",\n",
    "    10: \"Troitsk\",\n",
    "    9: \"Zelenograd\",\n",
    "    12: \"Other_district\"\n",
    "}\n",
    "material_categories = {\n",
    "    0: \"Bricks\",\n",
    "    1: \"Wood\",\n",
    "    2: \"Monolith\",\n",
    "    3: \"Panel\",\n",
    "    4: \"Block\",\n",
    "    5: \"Monolithic_brick\",\n",
    "    6: \"Stalin_project\",\n",
    "    7: \"Other_material\"\n",
    "}\n",
    "heating_categories = {\n",
    "    0: \"Central\",\n",
    "    1: \"Individual\",\n",
    "    2: \"Boiler\",\n",
    "    3: \"Autonomous_boiler\",\n",
    "    4: \"Other_heating\"\n",
    "}\n",
    "parking_categories = {\n",
    "    0: \"Ground\",\n",
    "    1: \"Underground\",\n",
    "    2: \"Multilevel\",\n",
    "    3: \"No_parking\"\n",
    "}\n",
    "seller_categories = {\n",
    "    0: \"Owner\",\n",
    "    1: \"Company\",\n",
    "    2: \"Agents\",\n",
    "    3: \"Developer\",\n",
    "    4: \"Other_seller\"\n",
    "}\n",
    "layout_categoires = {\n",
    "    0: \"Adjacent\",\n",
    "    1: \"Isolated\",\n",
    "    2: \"Adjacent_isolated\",\n",
    "    3: \"Other_layout\"\n",
    "}\n",
    "condition_categories = {\n",
    "    0: \"Undecorated\",\n",
    "    1: \"Decorated\",\n",
    "    2: \"Euro_repair\",\n",
    "    3: \"Special_design\",\n",
    "    4: \"Other_condition\"\n",
    "}\n",
    "categorical_features = {\n",
    "    \"parking\": parking_categories,\n",
    "    \"heating\": heating_categories,\n",
    "    \"material\": material_categories,\n",
    "    \"district\": district_categories,\n",
    "    \"seller\": seller_categories,\n",
    "    \"layout\": layout_categoires,\n",
    "    \"condition\": condition_categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cd5119f5-76df-4086-99a3-8db469089220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_categorical_features(data, remove_org=True):\n",
    "    for orig_feat, categories in categorical_features.items():\n",
    "        for k, category in categories.items():\n",
    "            data[category] = (data[orig_feat] == k).astype(int)\n",
    "        if remove_org:\n",
    "            data.drop(orig_feat, axis=1, inplace=True)\n",
    "one_hot_encode_categorical_features(train_data)\n",
    "one_hot_encode_categorical_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755673bc-b62b-4b7d-999c-d4b5f031ec1b",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4c014e4-006b-4538-b554-204ce4950d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['area_total', 'floor', 'rooms', 'ceiling', 'bathrooms_shared',\n",
    "       'bathrooms_private', 'windows_court', 'windows_street', 'balconies',\n",
    "       'loggias', 'phones', 'new', 'latitude', 'longitude', 'constructed',\n",
    "       'stories', 'elevator_without', 'elevator_passenger', 'elevator_service',\n",
    "       'garbage_chute', 'distance_to_center', 'distance_to_metro',\n",
    "       'metro_line_1', 'metro_line_2', 'metro_line_3', 'metro_line_4',\n",
    "       'metro_line_5', 'metro_line_6', 'metro_line_7', 'metro_line_8',\n",
    "       'metro_line_9', 'metro_line_10', 'metro_line_11', 'metro_line_14',\n",
    "       'number_of_metro_lines', 'bearing', 'distance_to_hospital',\n",
    "       'distance_to_state_uni', 'distance_to_tech_uni', 'Ground',\n",
    "       'Underground', 'Multilevel', 'No_parking', 'Central',\n",
    "       'Autonomous_boiler', 'Other_heating', 'Bricks', 'Monolith', 'Panel',\n",
    "       'Other_material', 'North-West', 'Novomoskovsk', 'Owner', 'Company',\n",
    "       'Agents', 'Developer', 'Other_seller', 'Isolated', 'Other_layout',\n",
    "       'Undecorated', 'Decorated', 'Euro_repair', 'Special_design',\n",
    "       'Other_condition'] # RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c5f93025-a0c8-4de4-adb2-a66e30a29572",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns\n",
    "features = features.drop([\"id\", \"building_id\", \"street\", \"address\", \"price\", \"log_price\", \"price_per_m2\"], errors=\"ignore\")\n",
    "features = features.drop(['area_kitchen', 'area_living'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289eb44-f2a5-427d-ad18-c12f97a62bd3",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "342726bb-08c1-4eb2-b63a-d50a1907ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before: 23285 | Samples after: 23078\n"
     ]
    }
   ],
   "source": [
    "def remove_min_max_outliers(train_data, test_data, features):\n",
    "    t = train_data.copy()\n",
    "    test_feat_min_max = pd.DataFrame({\"max\": test_data.describe().loc[\"max\"], \"min\": test_data.describe().loc[\"min\"]}).T\n",
    "    for feature in features:\n",
    "        max_val = test_feat_min_max[feature][\"max\"]\n",
    "        min_val = test_feat_min_max[feature][\"min\"]\n",
    "        max_ind = t[t[feature] > max_val].index\n",
    "        min_ind = t[t[feature] < min_val].index\n",
    "        t.drop(max_ind, inplace=True)\n",
    "        t.drop(min_ind, inplace=True)\n",
    "\n",
    "    print(\"Samples before: {} | Samples after: {}\".format(len(train_data), len(t)))\n",
    "    return t\n",
    "\n",
    "train_data = remove_min_max_outliers(train_data, test_data, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6096f2-f09c-456a-8eda-ff1ed3ee56b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8ecc2c5e-18fe-4aef-a5b9-5b32af0d21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "075ab44d-8dac-47f3-9114-bb8346aea152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model1 = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    criterion='mse',\n",
    "    n_jobs=2,\n",
    "    random_state=42,\n",
    "    max_features=\"log2\",\n",
    "    bootstrap=False,\n",
    "    max_depth=18,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=1.6746306849395426e-07\n",
    ")\n",
    "\n",
    "model2 = lgbm.LGBMRegressor(\n",
    "    random_state=42,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=20,\n",
    "    min_data_in_leaf=12,\n",
    "    max_depth=20,\n",
    "    max_bin=160,\n",
    ")\n",
    "\n",
    "model3 = CatBoostRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.2,\n",
    "    l2_leaf_reg=0.0001351899206097861,\n",
    "    bagging_temperature=8,\n",
    "    min_data_in_leaf=14,\n",
    "    thread_count=2,\n",
    "    depth=7,\n",
    "    silent=True,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "model4 = GradientBoostingRegressor(\n",
    "    learning_rate=0.2,\n",
    "    n_estimators=100,\n",
    "    criterion='friedman_mse',\n",
    "    subsample=0.9,\n",
    "    max_depth=10,\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"random_forest\": model1,\n",
    "    \"lgbm\": model2,\n",
    "    \"cat_boost\": model3,\n",
    "    \"gbm\": model4,\n",
    "}\n",
    "\n",
    "model_targets = {\n",
    "    \"random_forest\": \"log_price\",\n",
    "    \"lgbm\": \"log_price\",\n",
    "    \"cat_boost\": \"log_price\",\n",
    "    \"gbm\": \"log_price\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "93e2638b-a632-462d-b620-e47d6d2e9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cross_validation(model, k, data, features, target_feature):\n",
    "    kf = KFold(\n",
    "        n_splits=k,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(kf.split(buildings_train)), total=k):\n",
    "        building_val_ids = buildings_train.id.values[test_index]\n",
    "        train = data[~data.building_id.isin(building_val_ids)]\n",
    "        val = data[data.building_id.isin(building_val_ids)]\n",
    "        X_train = train[features].values\n",
    "        X_val = val[features].values\n",
    "        y_train = train[[target_feature]].values.ravel()\n",
    "        y_val = val[[target_feature]].values.ravel()\n",
    "        y_train_true = train[[\"price\"]].values.ravel()\n",
    "        y_val_true = val[[\"price\"]].values.ravel()\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        train_pred = model.predict(X_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        if target_feature == \"log_price\":\n",
    "            train_pred = (np.e ** train_pred) - 1\n",
    "            val_pred = (np.e ** val_pred) - 1\n",
    "        elif target_feature == \"price_per_m2\":\n",
    "            train_pred = train_pred * train.area_total\n",
    "            val_pred = val_pred * val.area_total\n",
    "        else:\n",
    "            raise Exception(\"Unknown target feature\")\n",
    "            \n",
    "        train_rsmle = root_mean_squared_log_error(y_true=y_train_true, y_pred=train_pred)\n",
    "        val_rsmle = root_mean_squared_log_error(y_true=y_val_true, y_pred=val_pred)\n",
    "        train_scores.append(train_rsmle)\n",
    "        val_scores.append(val_rsmle)\n",
    "    train_scores = np.array(train_scores)\n",
    "    val_scores = np.array(val_scores)\n",
    "    print(\"Train rsmle: {} | Std: {}\".format(train_scores.mean(), train_scores.std()))\n",
    "    print(\"Val rsmle: {} | Std: {}\".format(val_scores.mean(), val_scores.std()))\n",
    "    return val_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ca9ffac9-ba6f-4375-958e-b967623536f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.0752276053003107 | Std: 0.0008927849952340296\n",
      "Val rsmle: 0.21642288010409816 | Std: 0.022130467751712058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████████▍                                                                                                                                                             | 1/5 [00:03<00:15,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████████████████████████████████████▊                                                                                                                      | 2/5 [00:07<00:11,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                              | 3/5 [00:11<00:07,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                       | 4/5 [00:14<00:03,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.043628036746660395 | Std: 0.0008055537073811641\n",
      "Val rsmle: 0.20646921095851742 | Std: 0.024254198430964498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:06<00:00, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.02247490970726406 | Std: 0.00043028513267843094\n",
      "Val rsmle: 0.2105702452872879 | Std: 0.024930094913785433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:49<00:00, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.027249789529888123 | Std: 0.0008217964809809456\n",
      "Val rsmle: 0.2281358517268775 | Std: 0.02264727813273152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1_scores = cross_validation(model1, 5, train_data, features, \"price_per_m2\")\n",
    "model2_scores = cross_validation(model2, 5, train_data, features, \"log_price\")\n",
    "model3_scores = cross_validation(model3, 5, train_data, features, \"log_price\")\n",
    "model4_scores = cross_validation(model4, 5, train_data, features, \"log_price\")\n",
    "scores = {\n",
    "    \"rf\": model1_scores,\n",
    "    \"lgbm\": model2_scores,\n",
    "    \"cb\": model3_scores,\n",
    "    \"gbm\": model4_scores\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e1032472-c666-4b1e-9135-5d22bd69e884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsmle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.216423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.206469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>0.210570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbm</th>\n",
       "      <td>0.228136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rsmle\n",
       "rf    0.216423\n",
       "lgbm  0.206469\n",
       "cb    0.210570\n",
       "gbm   0.228136"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame(scores, index=[0])\n",
    "model_scores = model_scores.T\n",
    "model_scores.columns = [\"rsmle\"]\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d10a8-e0fc-432b-83e3-a73cfb14486a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc70d1f2-73f6-4567-bbd2-d44b3f283c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction using stacking with averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "612733ee-b17a-4c1a-88e4-f90e9ae8b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    }
   ],
   "source": [
    "f_lgbm = features\n",
    "model1.fit(train_data[f_lgbm], train_data[\"price_per_m2\"])\n",
    "model2.fit(train_data[f_lgbm], train_data[\"log_price\"])\n",
    "model3.fit(train_data[f_lgbm], train_data[\"log_price\"])\n",
    "model4.fit(train_data[f_lgbm], train_data[\"log_price\"])\n",
    "\n",
    "model1_pred = model1.predict(test_data[f_lgbm])\n",
    "model2_pred = model2.predict(test_data[f_lgbm])\n",
    "model3_pred = model3.predict(test_data[f_lgbm])\n",
    "model4_pred = model4.predict(test_data[f_lgbm])\n",
    "\n",
    "model1_pred *= test_data.area_total\n",
    "model2_pred = (np.e ** model2_pred) - 1\n",
    "model3_pred = (np.e ** model3_pred) - 1\n",
    "model4_pred = (np.e ** model4_pred) - 1\n",
    "\n",
    "predictions = [model1_pred, model2_pred, model3_pred, model4_pred]\n",
    "pred = np.average(predictions, \n",
    "    weights = 1 / model_scores['rsmle'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f12ab-4679-45ba-b017-a934e9db59b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prediction using stacking with meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "741ebac8-ac7c-46fe-bfec-cbe02e3b6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### funksjonene må oppdateres #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1db3d786-de7a-4c3f-9c4f-5f59b358add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, train_data, test_data, features, target_feature, k=5):\n",
    "    \"\"\"\n",
    "    Popular function on Kaggle.\n",
    "    \n",
    "    Trains a classifier on 4/5 of the training data and\n",
    "    predicts the rest (1/5). This procedure is repeated for all 5 folds,\n",
    "    thus we have predictions for all training set. This prediction is one\n",
    "    column of meta-data, later on used as a feature column by a meta-algorithm.\n",
    "    We predict the test part and average predictions across all 5 models.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    clf -- classifier\n",
    "    x_train -- 4/5 of training data\n",
    "    y_train -- corresponding labels\n",
    "    x_test -- all test data\n",
    "    \n",
    "    \"\"\"\n",
    "    oof_train = np.zeros((train_data.shape[0],))\n",
    "    oof_test = np.zeros((test_data.shape[0],))\n",
    "    oof_test_skf = np.empty((k, test_data.shape[0]))\n",
    "    \n",
    "    kf = KFold(\n",
    "        n_splits=k,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    buildings = buildings_train[buildings_train.id.isin(train_data.building_id)]\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(buildings)):\n",
    "        building_val_ids = buildings.id.values[test_index]\n",
    "        train = train_data[~train_data.building_id.isin(building_val_ids)]\n",
    "        val = train_data[train_data.building_id.isin(building_val_ids)]\n",
    "        X_train = train[features].values\n",
    "        X_val = val[features].values\n",
    "        y_train = train[[target_feature]].values.ravel()\n",
    "        y_val = val[[target_feature]].values.ravel()\n",
    "        y_train_true = train[[\"price\"]].values.ravel()\n",
    "        y_val_true = val[[\"price\"]].values.ravel()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        oof_train[test_index] = clf.predict(X_val)\n",
    "        oof_test_skf[i, :] = clf.predict(test_data[features].values)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "225ab14a-65c7-4bdf-bc1d-d528efbebb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, meta_model, X):\n",
    "    meta_prediction = np.zeros((X.shape[0], len(models)))\n",
    "    for i, model in enumerate(models):\n",
    "        meta_prediction[:, i] = model.predict(X)\n",
    "    return meta_model.predict(meta_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc8090bb-f2b9-4ef0-8fba-ea5e0e8fbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_data, features, target_features, split_ratio=0.2):\n",
    "    sample_count = round(split_ratio * buildings_train.shape[0])\n",
    "    buildings = buildings_train.sample(sample_count)\n",
    "    train = train_data[~train_data.building_id.isin(buildings.id)]\n",
    "    val = train_data[train_data.building_id.isin(buildings.id)]\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2da80965-1024-471c-9fbf-f91f11b0cbfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (3586,) could not be broadcast to indexing result of shape (1084,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4117/2328116905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"log_price\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"price_per_m2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"price\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel1_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1_oof_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_price\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel2_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2_oof_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_price\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel3_oof_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3_oof_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_price\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4117/3648732387.py\u001b[0m in \u001b[0;36mget_oof\u001b[0;34m(clf, train_data, test_data, features, target_feature, k)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moof_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0moof_test_skf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (3586,) could not be broadcast to indexing result of shape (1084,)"
     ]
    }
   ],
   "source": [
    "tr, te = train_val_split(train_data, features, [\"log_price\", \"price_per_m2\", \"price\"])\n",
    "\n",
    "model1_oof_train, model1_oof_val = get_oof(model1, tr, te, features, \"log_price\")\n",
    "model2_oof_train, model2_oof_val = get_oof(model2, tr, te, features, \"log_price\")\n",
    "model3_oof_train, model3_oof_val = get_oof(model3, tr, te, features, \"log_price\")\n",
    "model4_oof_train, model4_oof_val = get_oof(model4, tr, te, features, \"log_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41890a52-a48c-4da7-8924-562c565f499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((\n",
    "    rf_oof_train,\n",
    "    lgbm_oof_train,\n",
    "    cb_oof_train,\n",
    "    gbm_oof_train\n",
    "), axis=1)\n",
    "\n",
    "x_val = np.concatenate((\n",
    "    rf_oof_val,\n",
    "    lgbm_oof_val,\n",
    "    cb_oof_val,\n",
    "    gbm_oof_val\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25587e-bc16-4b3f-b13b-bfbb98f7db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_MODEL = lgbm.LGBMRegressor(\n",
    "    random_state=SEED,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=30\n",
    ")\n",
    "\n",
    "# fit with estimations of training data and decrease the error in predicting the true data\n",
    "META_MODEL.fit(x_train, y_train)\n",
    "# predict the test data based on the output of the other models\n",
    "pred = META_MODEL.predict(x_val)\n",
    "\n",
    "rsmle = root_mean_squared_log_error(y_true=(np.e ** y_val) - 1, y_pred=(np.e ** pred) - 1)\n",
    "print(f'Valid rmsle: {rsmle :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52da768-4028-4672-ad87-26e61fb8b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_predict([model1, model2, model3, model4], META_MODEL, X_test)\n",
    "pred = (np.e ** pred) - 1\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission_stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449f70e-4920-4796-ac84-a8e5ecd2b517",
   "metadata": {},
   "source": [
    "# Feature selection experimental"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
