{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc90fc-b581-4d9c-aec0-dfaee728d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d9d93-afa7-4dea-a014-34f3eb0cb3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea19ac-1770-43f6-8578-60cb3342e7c2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbc63a8-2e3a-4b00-9b8d-ef7abd01113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments_train = pd.read_csv('data/apartments_train.csv')\n",
    "buildings_train = pd.read_csv('data/buildings_train.csv')\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "\n",
    "train = pd.merge(\n",
    "    apartments_train, \n",
    "    buildings_train.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")\n",
    "test = pd.merge(\n",
    "    apartments_test, \n",
    "    buildings_test.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2bdce-fbf2-40da-a78a-c58a1be54e16",
   "metadata": {},
   "source": [
    "## Replace missing coordinates and districts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8f084-2d6a-4273-a5b4-b00d10dc38c9",
   "metadata": {},
   "source": [
    "**EDA insight:** TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4dbd51-efc0-4c8d-94d5-c8ce941ea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = test[test[\"longitude\"].isna()].index\n",
    "test.at[ind, \"latitude\"] = 55.56776345324702\n",
    "test.at[ind, \"longitude\"] = 37.48171529826662"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad731383-6e04-4ce7-bc3c-6aecaff0a133",
   "metadata": {},
   "source": [
    "**EDA insight:** TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4376456d-cf66-4482-9f4a-47b925077a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train[train.district.isna()].index\n",
    "train.at[ind, \"district\"] = 12\n",
    "ind = test[test.district.isna()].index\n",
    "test.at[ind, \"district\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baac6a5-b58e-4fe1-8ad1-0a64f44722f9",
   "metadata": {},
   "source": [
    "## Fixing incorrect coordinates in testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af011f-e640-4b51-8506-1bae6761f968",
   "metadata": {},
   "source": [
    "**EDA insight:** TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8927b11e-f046-4548-b769-e53c70da927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartment_coordinates = {\n",
    "    2511: [55.5438629741104, 37.48233890768276],\n",
    "    2529: [55.66866253043727, 37.217152651528785],\n",
    "    4719: [55.63132797843279, 37.426744466015705],\n",
    "    5090: [55.54390546580924, 37.48229599441532],\n",
    "    6959: [55.54390546580924, 37.48229599441532],\n",
    "    8596: [55.54390546580924, 37.48229599441532],\n",
    "    9547: [55.63399775464791, 37.41982879208156]\n",
    "}\n",
    "for k, v in apartment_coordinates.items():\n",
    "    test.at[k, \"latitude\"] = v[0]\n",
    "    test.at[k, \"longitude\"] = v[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269d61f-0540-454f-862c-9b504690a3fb",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f434d8d8-fc62-4d30-99c2-f32f8c2621b9",
   "metadata": {},
   "source": [
    "## Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e081cc2a-42ac-4fab-9a02-e8b06f087656",
   "metadata": {},
   "source": [
    "### Expensive districts\n",
    "There are a range of districts within Moscow that are notorious for expensive apartments according to [Russia Beyond](https://www.rbth.com/travel/332318-expensive-rich-districts-moscow), including Khamovniki, Yakimanka, Arbat, Presnensky, Tverskoy, Meschansky, and Zamoskvorechye. Based on this information,\n",
    "we created a set of features (is_in_khamovniki, is_in_yakimanka, etc.) using the polygon coordinates for each district obtained from open street map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35c0b7b-313a-4d77-b3e3-be909eaabc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wealthy_districts_features(data):\n",
    "    with open(\"data/geojson/khamovniki_polygon_data.geojson\") as f:\n",
    "        gj_khamovniki = geojson.load(f)\n",
    "    with open(\"data/geojson/yakimanka_polygon_data.geojson\") as f:\n",
    "        gj_yakimanka = geojson.load(f)\n",
    "    with open(\"data/geojson/arbat_polygon_data.geojson\") as f:\n",
    "        gj_arbat = geojson.load(f)\n",
    "    with open(\"data/geojson/presnensky_polygon_data.geojson\") as f:\n",
    "        gj_presnensky = geojson.load(f)\n",
    "    with open(\"data/geojson/Tverskoy_polygon_data.geojson\") as f:\n",
    "        gj_tverskoy = geojson.load(f)\n",
    "    khamovniki_polygon = Polygon(gj_khamovniki[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    yakimanka_polygon = Polygon(gj_yakimanka[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    arbat_polygon = Polygon(gj_arbat[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    presnensky_polygon = Polygon(gj_presnensky[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    tverskoy_polygon = Polygon(gj_tverskoy[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    for ind, row in data.iterrows():\n",
    "        point = Point([row[\"longitude\"], row[\"latitude\"]])\n",
    "        data.at[ind, \"is_in_khamovniki\"] = 1 if khamovniki_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_yakimanka\"] = 1 if yakimanka_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_arbat\"] = 1 if arbat_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_presnensky\"] = 1 if presnensky_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_tverskoy\"] = 1 if tverskoy_polygon.contains(point) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3008159a-7d4c-4427-b8c0-343a6ede69fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metro stations\n",
    "Moscow has a sophisticated metro network comprising 15 lines. Using a public list on [Wikipedia](https://en.wikipedia.org/wiki/List_of_Moscow_Metro_stations) comprising all stations and transitions, we constructed a wide range of features, including the distance to the nearest station, the number of lines that can be reached from that station, and whether or not a specific metro line is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6595f4c-371c-4470-8020-89579d8236cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_metro_feature(data, add_metro_lines=True):\n",
    "    d = data.copy()\n",
    "\n",
    "    metro_data = pd.read_csv(\"data/moscow_metro_data.csv\", delimiter=\";\")\n",
    "\n",
    "    # drop duplicates\n",
    "    ind = metro_data[metro_data[\"English transcription\"].duplicated()].index\n",
    "    metro_data = metro_data.drop(ind)\n",
    "    metro_data = metro_data.reset_index()\n",
    "    metro_lines = [\"metro_line_{}\".format(i) for i in range(1, 16)]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(metro_data[[\"latitude\", \"longitude\"]])\n",
    "    distances, indices = nbrs.kneighbors(d[[\"latitude\", \"longitude\"]])\n",
    "    d[\"metro_index\"] = indices\n",
    "\n",
    "    for ind, row in d.iterrows():\n",
    "        metro_coordinate = metro_data.loc[[row.metro_index]][[\"latitude\", \"longitude\"]].to_numpy()\n",
    "        distance = geopy.distance.distance([[row[\"latitude\"], row[\"longitude\"]]], metro_coordinate).km\n",
    "        data.at[ind, \"distance_to_metro\"] = distance\n",
    "\n",
    "        if add_metro_lines:\n",
    "            number_of_metro_lines = 0\n",
    "            metro_lines_data = metro_data.loc[[row.metro_index]][metro_lines]\n",
    "            for metro_line in metro_lines:\n",
    "                access_to_line = metro_lines_data[metro_line].values[0]\n",
    "                data.at[ind, metro_line] = access_to_line\n",
    "                if access_to_line == 1:\n",
    "                    number_of_metro_lines += 1\n",
    "            data.at[ind, \"number_of_metro_lines\"] = number_of_metro_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfc3a9-b63e-43f3-85c3-6394dab8c05d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hospitals\n",
    "Using data from open street map, we computed the distance to the nearest hospital, which had a positive effect on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d1ac44b-de0c-485b-a4cd-2c5f1880a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_hospital_feature(data):\n",
    "    d = data.copy()\n",
    "    hospital_data = pd.read_csv(\"data/hospitals.csv\")\n",
    "    hospital_data = hospital_data[[\"latitude\", \"longitude\"]]\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(hospital_data)\n",
    "    distances, indices = nbrs.kneighbors(data[[\"latitude\", \"longitude\"]])\n",
    "    d[\"hospital_index\"] = indices\n",
    "    distances = []\n",
    "    for ind, row in d.iterrows():\n",
    "        apartment_coordinates = [[row[\"latitude\"], row[\"longitude\"]]]\n",
    "        hospital_coordinate = hospital_data.loc[[row.hospital_index]].to_numpy()\n",
    "        distance = geopy.distance.distance(apartment_coordinates, hospital_coordinate).km\n",
    "        distances.append(distance)\n",
    "    data[\"distance_to_hospital\"] = distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac405ac-0729-4355-abc3-e090e2ba9f25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Universities\n",
    "Moscow has a great number of universities. However, two of the most important universities are the Moscow State University and the Technical University of Moscow. Apartments close to the universities have a tendency to increase in price. Hence, we added a feature measuring the distance to both universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfe680f-98ee-480f-ad3e-7d4abfa7a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_universities_feature(data):\n",
    "    state_uni_coordinates = [55.70444300116007, 37.528611852796914]\n",
    "    tech_uni_coordinates = [55.76666597872545, 37.68511242319504]\n",
    "    distances_to_state_uni = []\n",
    "    distances_to_tech_uni = []\n",
    "    for ind, row in data.iterrows():\n",
    "        distances_to_state_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], state_uni_coordinates).km)\n",
    "        distances_to_tech_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], tech_uni_coordinates).km)\n",
    "    data[\"distance_to_state_uni\"] = distances_to_state_uni\n",
    "    data[\"distance_to_tech_uni\"] = distances_to_tech_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cc39c-c83c-4117-9101-cd055aeb63e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Distance to city center\n",
    "Apartments closer to the city center often have a higher price than those further away. Hence, we created a distance_to_center feature based on latitude and longitude giving the distance to the center of moscow in kilometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf4a7c4-da99-4477-abf4-df3f95205cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_center_feature(data):\n",
    "    moscow_center = [55.751244, 37.618423]\n",
    "    coordinates = data[['latitude', 'longitude']].to_numpy()\n",
    "    dist = [geopy.distance.distance(moscow_center, coordinate).km for coordinate in coordinates]\n",
    "    data['distance_to_center'] = dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c2444-8a0a-456d-a994-9081697669cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Bearing\n",
    "We noticed that apartments in the center and western part of moscow tend to have higher prices than others. Thus, we calculated the bearing from the city center based on the latitude and longitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccdd1d1-2503-4dfe-b274-6e94dc19a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearing_feature(data):\n",
    "    for ind, row in data.iterrows():\n",
    "        moscow_center = [55.751244, 37.618423]\n",
    "        c1 = moscow_center\n",
    "        c2 = [row[\"latitude\"], row[\"longitude\"]]\n",
    "        y = np.sin(c2[1] - c1[1]) * np.cos(c2[0])\n",
    "        x = np.cos(c1[0]) * np.sin(c2[0]) - np.sin(c1[0]) * np.cos(c2[0]) * np.cos(c2[1] - c1[1])\n",
    "        rad = np.arctan2(y, x)\n",
    "        bearing = ((rad * 180 / np.pi) + 360) % 360\n",
    "        data.at[ind, \"bearing\"] = bearing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a2fab-18e2-41ce-850d-011d947c9880",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Box areas\n",
    "We further split the map into different coordinate tiles to better capture the more expensive apartments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e392b68d-0753-417b-8193-4a3272c498a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_box_areas(data):\n",
    "    locs_with_high_corr = ['loc_37.625_55.725',\n",
    "                           'loc_37.6_55.725',\n",
    "                           'loc_37.6_55.75',\n",
    "                           'loc_37.575_55.7',\n",
    "                           'loc_37.575_55.725',\n",
    "                           'loc_37.575_55.75',\n",
    "                           'loc_37.55_55.7',\n",
    "                           'loc_37.55_55.725']\n",
    "    for lon in np.arange(37.45, 37.65, 0.025):\n",
    "        for lat in np.arange(55.6, 55.8, 0.025):\n",
    "            column_name = \"loc_\" + str(round(lon, 3)) + \"_\" + str(round(lat, 3))\n",
    "            if column_name in locs_with_high_corr:\n",
    "                data[column_name] = 0\n",
    "                indexes = data[\n",
    "                    (lon < data['longitude']) & (data['longitude'] < lon + 0.025) & (lat < data['latitude']) & (\n",
    "                                data['latitude'] < lat + 0.025)\n",
    "                    ].index\n",
    "\n",
    "                data.loc[indexes, column_name] = 1\n",
    "                \n",
    "def add_box_areas_more_boxes(data):\n",
    "    \"\"\"only add values in list to prevent unnecessary slowdown\"\"\"\n",
    "    locs_with_high_corr = [\n",
    "            'loc_37.575_55.75', 'loc_37.6_55.725', 'loc_37.525_55.75',\n",
    "           'loc_37.55_55.775', 'loc_37.525_55.725', 'loc_37.575_55.725',\n",
    "           'loc_37.525_55.7', 'loc_37.5_55.7', 'loc_37.55_55.725',\n",
    "           'loc_37.475_55.7', 'loc_37.625_55.725', 'loc_37.5_55.75',\n",
    "           'loc_37.525_55.775', 'loc_37.525_55.675', 'loc_37.55_55.675',\n",
    "           'loc_37.55_55.75', 'loc_37.4_55.6', 'loc_37.725_55.575',\n",
    "           'loc_37.925_55.675', 'loc_37.475_55.55', 'loc_37.65_55.575',\n",
    "           'loc_37.525_55.875', 'loc_37.5_55.55', 'loc_37.475_55.525',\n",
    "           'loc_37.925_55.7'\n",
    "    ]\n",
    "    for lon in np.arange(37.4, 37.95, 0.025):\n",
    "        for lat in np.arange(55.525, 55.9, 0.025):\n",
    "            column_name = \"loc_\" + str(round(lon, 3)) + \"_\" + str(round(lat, 3))\n",
    "            if column_name in locs_with_high_corr: #column_name in locs_with_high_corr:\n",
    "                data[column_name] = 0\n",
    "                indexes = data[\n",
    "                    (lon < data['longitude']) & (data['longitude'] < lon + 0.025) & (lat < data['latitude']) & (\n",
    "                                data['latitude'] < lat + 0.025)\n",
    "                    ].index\n",
    "\n",
    "                data.loc[indexes, column_name] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c154d-5c8e-41fb-8ff3-6a3d8d2bf967",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Understand how the data was generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70190c39-1cc0-43d6-8f88-62df8f23a9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_buildings = set(train.building_id.values)\n",
    "test_buildings = set(test.building_id.values)\n",
    "venn2([train_buildings, test_buildings])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5641e458-9a8b-484f-ae68-7737a81ae369",
   "metadata": {},
   "source": [
    "As we can see from the venn diagram above, there is no overlap between the building_ids in the training and test data, showing that the buildings in the test data are completely different from those in the training data. Hence, in order to obtain accurate results during model validation, the training set should be split in a validation and training set based on the same assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c534ea-e5c2-497c-ac56-61362118edc5",
   "metadata": {},
   "source": [
    "## Explore individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031819f8-7d31-41d5-89a0-930931642ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df33d80f-8ddf-4812-9b9b-2b663e104c1e",
   "metadata": {},
   "source": [
    "## Explore pairs and groups of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba79ac6-aa2e-4f93-8229-e5aa394f62c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "178cf987-0173-4207-8cb9-9925b48c2bf5",
   "metadata": {},
   "source": [
    "## Clean up features - Data imputation\n",
    "We imputed all features in the data provided using different approaches for boolean, categorical, and numerical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70485b0-6ba7-4cc3-801b-b1a477cedae2",
   "metadata": {},
   "source": [
    "### Categorical features\n",
    "In case of categorical features, we simply added a new category for all rows with nans. For instance, in the catgory 'seller', nan values were replaced with category 4 denoting 'other_seller'.\n",
    "\n",
    "We made an exception for 'windows_street' and 'windows_court' since we cannot assume that all the other buildings don't have any windows to the street or court if this feature contains nans. Hence, we used the bins approach as explained below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38993fb-c97d-4b46-b88c-4b7e94895c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical_features(data):\n",
    "    data.loc[:, \"seller\"] = data.loc[:, \"seller\"].fillna(4)\n",
    "    data.loc[:, \"layout\"] = data.loc[:, \"layout\"].fillna(3)\n",
    "    data.loc[:, \"condition\"] = data.loc[:, \"condition\"].fillna(4)\n",
    "    data.loc[:, \"material\"] = data.loc[:, \"material\"].fillna(7)\n",
    "    data.loc[:, \"parking\"] = data.loc[:, \"parking\"].fillna(3)\n",
    "    data.loc[:, \"heating\"] = data.loc[:, \"heating\"].fillna(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c540de9-4b0c-4076-87ad-9a3a25c48c5c",
   "metadata": {},
   "source": [
    "### Boolean features\n",
    "If a boolean feature contained a nan, we generally interpreted it as 'not present', i.e., we set it to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc84021e-7102-4d3c-b011-4f00874053cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_boolean_features(data):\n",
    "    data.loc[:, \"elevator_without\"] = data.loc[:, \"elevator_without\"].fillna(0)\n",
    "    data.loc[:, \"elevator_service\"] = data.loc[:, \"elevator_service\"].fillna(0)\n",
    "    data.loc[:, \"elevator_passenger\"] = data.loc[:, \"elevator_passenger\"].fillna(0)\n",
    "    data.loc[:, \"garbage_chute\"] = data.loc[:, \"garbage_chute\"].fillna(0)\n",
    "    data.loc[:, \"balconies\"] = data.loc[:, \"balconies\"].fillna(0)\n",
    "    data.loc[:, \"loggias\"] = data.loc[:, \"loggias\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03740eaf-e090-48a6-a814-958d3737fe28",
   "metadata": {},
   "source": [
    "### Numerical features\n",
    "In case of numerical features, we tried to avoid to simply set it to the mean value as this would probably lead to a distortion of the real data. Instead, we imputed it using bins based on area_total. We computed k bins based on area_total, calculated the mean for a given feature in the corresponding bin, and replaced nans in this range with the computed value.\n",
    "\n",
    "An exception is constructed. Before imputing it based on bins, we set constructed to 2021 if new was True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d840e1a7-180f-453d-be1c-3b06b6745a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_bin_mean(data, train_data, test_data, feature, bin_feature, bins=40, decimals=0, verbose=False):\n",
    "    tr = train_data.copy()\n",
    "    te = test_data.copy()\n",
    "    database = pd.concat([tr, te])\n",
    "    database[\"bin\"], bins = pd.cut(database[bin_feature], bins=40, retbins=True)\n",
    "    pb = 0\n",
    "    for b in bins:\n",
    "        bin_data = database[(database[bin_feature] >= pb) & (database[bin_feature] <= b)]\n",
    "        bin_mean = bin_data[feature].mean()\n",
    "        percentage = (bin_mean / bin_data[bin_feature]).mean()\n",
    "        d = data[(data[bin_feature] >= pb) & (data[bin_feature] <= b)]\n",
    "        ind = d[d[feature].isna()].index\n",
    "        if pd.notna(bin_mean) and len(ind) > 0:\n",
    "            bin_mean = round(bin_mean, decimals)\n",
    "            data.at[ind, feature] = bin_mean\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Set {} for {} rows with {} <= {} <= {} to {}\".format(feature, len(ind), pb, bin_feature, b, bin_mean))\n",
    "        pb = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d033824-a750-4547-8e54-e55ccfe9099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numerical_features(data):\n",
    "    data.loc[data.new == 1, \"constructed\"] = data.loc[data.new == 1, \"constructed\"].fillna(2021)\n",
    "    impute_with_bin_mean(data=data, train_data=train, test_data=test, feature=\"constructed\", bins=12, bin_feature=\"district\")\n",
    "    data.loc[data.constructed >= 2021, \"new\"] = data.loc[data.constructed >= 2021, \"new\"].fillna(1)\n",
    "    data.loc[data.constructed < 2021, \"new\"] = data.loc[data.constructed < 2021, \"new\"].fillna(0)\n",
    "    \n",
    "    numerical_features = [\"phones\", \"ceiling\", \"bathrooms_shared\", \"bathrooms_private\", \"windows_street\", \"windows_court\", \"area_kitchen\", \"area_living\"]\n",
    "    for num_feat in numerical_features:\n",
    "        impute_with_bin_mean(data=data, train_data=train, test_data=test, feature=num_feat, bin_feature=\"area_total\", bins=20)\n",
    "        fill_value = data[data[num_feat].notna()][num_feat].mean(axis=0).round()\n",
    "        data.loc[:, num_feat] = data.loc[:, num_feat].fillna(fill_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936f4e0f-202b-4903-b423-c6d728dd72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nans(data):\n",
    "    impute_categorical_features(data)\n",
    "    impute_boolean_features(data)\n",
    "    impute_numerical_features(data)\n",
    "\n",
    "train_data = train_with_feats.copy()\n",
    "test_data = test_with_feats.copy()\n",
    "impute_nans(train_data)\n",
    "impute_nans(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db4c7e-1297-404c-b73d-ba40ec167164",
   "metadata": {},
   "source": [
    "domain knowledge: \n",
    "- there are some notorious expensive areas in moscow (refer to news article)\n",
    "- metro stations could play an important role and the lines that are accessible\n",
    "- hospitals may also be important\n",
    "\n",
    "understand how the data was generated:\n",
    "- show that the train and test was split based on building id\n",
    "\n",
    "explore individual features:\n",
    "- add plots from previous notebooks\n",
    "\n",
    "clean up features:\n",
    "- refer to data imputation methods\n",
    "\n",
    "explore pairs and groups:\n",
    "- analyse area_total with respect to area_kitchen, area_living, and rooms to show that they have a high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a5bac-acc9-411c-b834-7ed398a569c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "The new features that we added were explained extensively above in the section 'domain knowledge'. In addition to that, we also performed a log transformation of the price and computed the price per square meter as this improved model performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf37425-8a12-4473-bf3b-15124149510f",
   "metadata": {},
   "source": [
    "## Target features in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447d9a8d-bc1b-4528-bb01-06d844ce4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_price\"] = np.log1p(train.price)\n",
    "train[\"price_per_m2\"] = train.price / train.area_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39934d84-67ee-45ab-aef4-30c8b37872c9",
   "metadata": {},
   "source": [
    "## Feature construction - (NOTE: TAKES SOME TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e2cd22-1411-40a8-b927-db928bdc94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(data):\n",
    "    distance_to_center_feature(data)\n",
    "    distance_to_metro_feature(data)\n",
    "    bearing_feature(data)\n",
    "    distance_to_hospital_feature(data)\n",
    "    distance_to_universities_feature(data)\n",
    "    wealthy_districts_features(data)\n",
    "    add_box_areas_more_boxes(data)\n",
    "\n",
    "train_with_feats = train.copy()\n",
    "test_with_feats = test.copy()\n",
    "construct_features(train_with_feats)\n",
    "construct_features(test_with_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a8d72-3269-4743-817d-f1b10b01027b",
   "metadata": {},
   "source": [
    "# One-hot encoding categorical features\n",
    "We one-hot encoded all categorical features and dropped the original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "518f4b13-c4cf-47ac-b9ad-619b77375991",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_categories = {\n",
    "    3: \"East\",\n",
    "    6: \"South-West\",\n",
    "    5: \"South\",\n",
    "    4: \"South-East\",\n",
    "    0: \"Central\",\n",
    "    2: \"North-East\",\n",
    "    1: \"North\",\n",
    "    8: \"North-West\",\n",
    "    7: \"West\",\n",
    "    11: \"Novomoskovsk\",\n",
    "    10: \"Troitsk\",\n",
    "    9: \"Zelenograd\",\n",
    "    12: \"Other_district\"\n",
    "}\n",
    "material_categories = {\n",
    "    0: \"Bricks\",\n",
    "    1: \"Wood\",\n",
    "    2: \"Monolith\",\n",
    "    3: \"Panel\",\n",
    "    4: \"Block\",\n",
    "    5: \"Monolithic_brick\",\n",
    "    6: \"Stalin_project\",\n",
    "    7: \"Other_material\"\n",
    "}\n",
    "heating_categories = {\n",
    "    0: \"Central\",\n",
    "    1: \"Individual\",\n",
    "    2: \"Boiler\",\n",
    "    3: \"Autonomous_boiler\",\n",
    "    4: \"Other_heating\"\n",
    "}\n",
    "parking_categories = {\n",
    "    0: \"Ground\",\n",
    "    1: \"Underground\",\n",
    "    2: \"Multilevel\",\n",
    "    3: \"No_parking\"\n",
    "}\n",
    "seller_categories = {\n",
    "    0: \"Owner\",\n",
    "    1: \"Company\",\n",
    "    2: \"Agents\",\n",
    "    3: \"Developer\",\n",
    "    4: \"Other_seller\"\n",
    "}\n",
    "layout_categoires = {\n",
    "    0: \"Adjacent\",\n",
    "    1: \"Isolated\",\n",
    "    2: \"Adjacent_isolated\",\n",
    "    3: \"Other_layout\"\n",
    "}\n",
    "condition_categories = {\n",
    "    0: \"Undecorated\",\n",
    "    1: \"Decorated\",\n",
    "    2: \"Euro_repair\",\n",
    "    3: \"Special_design\",\n",
    "    4: \"Other_condition\"\n",
    "}\n",
    "categorical_features = {\n",
    "    \"parking\": parking_categories,\n",
    "    \"heating\": heating_categories,\n",
    "    \"material\": material_categories,\n",
    "    \"district\": district_categories,\n",
    "    \"seller\": seller_categories,\n",
    "    \"layout\": layout_categoires,\n",
    "    \"condition\": condition_categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd5119f5-76df-4086-99a3-8db469089220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_categorical_features(data, remove_org=True):\n",
    "    for orig_feat, categories in categorical_features.items():\n",
    "        for k, category in categories.items():\n",
    "            data[category] = (data[orig_feat] == k).astype(int)\n",
    "        if remove_org:\n",
    "            data.drop(orig_feat, axis=1, inplace=True)\n",
    "one_hot_encode_categorical_features(train_data)\n",
    "one_hot_encode_categorical_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289eb44-f2a5-427d-ad18-c12f97a62bd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c183f2-8748-407d-a2d5-9be72865eb2e",
   "metadata": {},
   "source": [
    "## Min-Max Outliers\n",
    "We analysed the test data with respect to the min and max values of each feature. If a row in the train data contains a value in one of the features that is greater than the min or max value, we removed the row from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "342726bb-08c1-4eb2-b63a-d50a1907ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before: 23285 | Samples after: 23063\n"
     ]
    }
   ],
   "source": [
    "def remove_min_max_outliers(train_data, test_data, features):\n",
    "    t = train_data.copy()\n",
    "    test_feat_min_max = pd.DataFrame({\"max\": test_data.describe().loc[\"max\"], \"min\": test_data.describe().loc[\"min\"]}).T\n",
    "    for feature in features:\n",
    "        max_val = test_feat_min_max[feature][\"max\"]\n",
    "        min_val = test_feat_min_max[feature][\"min\"]\n",
    "        max_ind = t[t[feature] > max_val].index\n",
    "        min_ind = t[t[feature] < min_val].index\n",
    "        t.drop(max_ind, inplace=True)\n",
    "        t.drop(min_ind, inplace=True)\n",
    "\n",
    "    print(\"Samples before: {} | Samples after: {}\".format(len(train_data), len(t)))\n",
    "    return t\n",
    "\n",
    "train_data = remove_min_max_outliers(train_data, test_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f23b07bb-35a6-4fe9-8b29-caa7831e8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6096f2-f09c-456a-8eda-ff1ed3ee56b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755673bc-b62b-4b7d-999c-d4b5f031ec1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature selection\n",
    "Selecting the features that should be part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f93025-a0c8-4de4-adb2-a66e30a29572",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_data.columns\n",
    "features = features.drop([\"id\", \"building_id\", \"street\", \"address\", \"price\", \"log_price\", \"price_per_m2\", \"street\", \"address\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ecc2c5e-18fe-4aef-a5b9-5b32af0d21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bac6e7-f1cc-4e60-8cdc-61132ea47b73",
   "metadata": {},
   "source": [
    "## Model 1: Stacking using averaging\n",
    "Our first model comprises LGBMRegressor, CatBoostRegressor, and a BaggingRegressor. We initially optimised the models using cross-validation to find the best hyperparameters. We saved the obtained scores of each model as this is required for the prediction to work. Thereafter, we trained the base models on the entire training set, predicted the prices of the test set with each model, and computed the weighted average with the weights based on the scores obtained in cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6949ce-0416-4c55-9ca9-ab6b760da909",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "075ab44d-8dac-47f3-9114-bb8346aea152",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "model2 = lgbm.LGBMRegressor(\n",
    "    random_state=SEED,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=20,\n",
    "    min_data_in_leaf=12,\n",
    "    max_depth=20,\n",
    "    max_bin=160,\n",
    ")\n",
    "\n",
    "model3 = CatBoostRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    thread_count=2,\n",
    "    depth=7,    \n",
    "    min_data_in_leaf=10,\n",
    "    silent=True,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "\n",
    "model5 = BaggingRegressor(\n",
    "    n_estimators=200,\n",
    "    n_jobs=2,\n",
    "    random_state=SEED,\n",
    "    max_features=0.7,\n",
    "    max_samples=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93e2638b-a632-462d-b620-e47d6d2e9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, k, data, features, target_feature):\n",
    "    kf = KFold(\n",
    "        n_splits=k,\n",
    "        shuffle=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(kf.split(buildings_train)), total=k):\n",
    "        building_val_ids = buildings_train.id.values[test_index]\n",
    "        train = data[~data.building_id.isin(building_val_ids)]\n",
    "        val = data[data.building_id.isin(building_val_ids)]\n",
    "        X_train = train[features].values\n",
    "        X_val = val[features].values\n",
    "        y_train = train[[target_feature]].values.ravel()\n",
    "        y_val = val[[target_feature]].values.ravel()\n",
    "        y_train_true = train[[\"price\"]].values.ravel()\n",
    "        y_val_true = val[[\"price\"]].values.ravel()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        train_pred = model.predict(X_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        if target_feature == \"log_price\":\n",
    "            train_pred = (np.e ** train_pred) - 1\n",
    "            val_pred = (np.e ** val_pred) - 1\n",
    "        elif target_feature == \"price_per_m2\":\n",
    "            train_pred = train_pred * train.area_total\n",
    "            val_pred = val_pred * val.area_total\n",
    "        else:\n",
    "            raise Exception(\"Unknown target feature\")\n",
    "            \n",
    "        train_rsmle = root_mean_squared_log_error(y_true=y_train_true, y_pred=train_pred)\n",
    "        val_rsmle = root_mean_squared_log_error(y_true=y_val_true, y_pred=val_pred)\n",
    "        train_scores.append(train_rsmle)\n",
    "        val_scores.append(val_rsmle)\n",
    "    train_scores = np.array(train_scores)\n",
    "    val_scores = np.array(val_scores)\n",
    "    print(\"Train rsmle: {} | Std: {}\".format(train_scores.mean(), train_scores.std()))\n",
    "    print(\"Val rsmle: {} | Std: {}\".format(val_scores.mean(), val_scores.std()))\n",
    "    return val_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca9ffac9-ba6f-4375-958e-b967623536f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▍                                                                                             | 1/5 [00:05<00:21,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████▊                                                                      | 2/5 [00:09<00:13,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████▏                                              | 3/5 [00:14<00:09,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 4/5 [00:21<00:05,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:28<00:00,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.043539145297335956 | Std: 0.0008279783609982373\n",
      "Val rsmle: 0.19780486182129559 | Std: 0.020068732311686478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f = features.copy()\n",
    "f = f.drop([\"area_living\", \"area_kitchen\", \"rooms\"])\n",
    "\n",
    "model2_scores = cross_validation(model2, 5, train_data, f, \"log_price\")\n",
    "model3_scores = cross_validation(model3, 5, train_data, f, \"log_price\")\n",
    "model5_scores = cross_validation(model5, 5, train_data, f, \"price_per_m2\")\n",
    "scores = {\n",
    "    \"lgbm\": model2_scores,\n",
    "    \"cb\": model3_scores,\n",
    "    \"bagging\": model5_scores,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1032472-c666-4b1e-9135-5d22bd69e884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsmle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.197805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rsmle\n",
       "lgbm  0.197805"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame(scores, index=[0])\n",
    "model_scores = model_scores.T\n",
    "model_scores.columns = [\"rsmle\"]\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f08f8e-08e4-4e29-a6f5-79b48a66b5fb",
   "metadata": {},
   "source": [
    "### Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70d1f2-73f6-4567-bbd2-d44b3f283c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prediction using model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "612733ee-b17a-4c1a-88e4-f90e9ae8b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of weights not compatible with specified axis.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qj/3xgy61cn4896krzkbpyn7gth0000gn/T/ipykernel_2620/2556486055.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel2_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel5_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m pred = np.average(predictions, \n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmodel_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rsmle'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/uni_dev/tdt4173_project/venv/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m    398\u001b[0m                     \"1D weights expected when shapes of a and weights differ.\")\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                     \"Length of weights not compatible with specified axis.\")\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of weights not compatible with specified axis."
     ]
    }
   ],
   "source": [
    "model2.fit(train_data[features], train_data[\"log_price\"])\n",
    "model3.fit(train_data[features], train_data[\"log_price\"])\n",
    "model5.fit(train_data[features], train_data[\"price_per_m2\"])\n",
    "\n",
    "model2_pred = model2.predict(test_data[features])\n",
    "model3_pred = model3.predict(test_data[features])\n",
    "model5_pred = model5.predict(test_data[features])\n",
    "\n",
    "model2_pred = (np.e ** model2_pred) - 1\n",
    "model3_pred = (np.e ** model3_pred) - 1\n",
    "model5_pred *= test_data.area_total\n",
    "\n",
    "predictions = [model2_pred, model3_pred, model5_pred]\n",
    "pred = np.average(predictions, \n",
    "    weights = 1 / model_scores['rsmle'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f12ab-4679-45ba-b017-a934e9db59b6",
   "metadata": {},
   "source": [
    "## Model 2: Stacking with meta model\n",
    "In our second model, we replaced the averaing step of model 1 with a meta model that tries to minimize the prediction errors of the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5accdc-7039-4376-a1bc-9ee9744c604c",
   "metadata": {},
   "source": [
    "### Training of the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ccbe7-7f37-4398-ba54-b80fa6a1fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv(data, building_data, k):\n",
    "    kf = KFold(\n",
    "        n_splits=k,\n",
    "        shuffle=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    inds = [ind for i, ind in enumerate(kf.split(building_data))]\n",
    "    i = 0\n",
    "    while i < len(inds):\n",
    "        building_val_ids = building_data.id.values[inds[i][1]]\n",
    "        train_ids = data[~data.building_id.isin(building_val_ids)].index.values\n",
    "        val_ids = data[data.building_id.isin(building_val_ids)].index.values\n",
    "        yield (train_ids, val_ids)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ccdcc56-82ef-4ab7-a3d6-e78e89004786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(data, buildings_data, features, target, ratio=0.2):\n",
    "    sample_count = round(0.2 * buildings_data.shape[0])\n",
    "    buildings_val = buildings_data.sample(sample_count)\n",
    "    buildings_train_data = buildings_data[~buildings_data.id.isin(buildings_val.id)]\n",
    "    X_train = data[~data.building_id.isin(buildings_val.id)]\n",
    "    X_val = data[data.building_id.isin(buildings_val.id)]\n",
    "    return X_train, X_val, buildings_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f31c86af-c316-4ce8-9c88-7126be8829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, buildings_train_data = train_val_split(train_data, buildings_train, f, \"log_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1db3d786-de7a-4c3f-9c4f-5f59b358add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, x_test, features, target, buildings_train_data):\n",
    "    \"\"\"\n",
    "    Popular function on Kaggle.\n",
    "    \n",
    "    Trains a classifier on 4/5 of the training data and\n",
    "    predicts the rest (1/5). This procedure is repeated for all 5 folds,\n",
    "    thus we have predictions for all training set. This prediction is one\n",
    "    column of meta-data, later on used as a feature column by a meta-algorithm.\n",
    "    We predict the test part and average predictions across all 5 models.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    clf -- classifier\n",
    "    x_train -- 4/5 of training data\n",
    "    y_train -- corresponding labels\n",
    "    x_test -- all test data\n",
    "    \n",
    "    \"\"\"\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((5, ntest))\n",
    "    x_train = x_train.reset_index()\n",
    "    x_test = x_test.reset_index()\n",
    "\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(custom_cv(x_train, buildings_train_data, 5)), total=5):\n",
    "        x_tr = x_train.loc[train_index, features]\n",
    "        y_tr = x_train.loc[train_index, target]\n",
    "        x_te = x_train.loc[test_index, features]\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test[features])\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "225ab14a-65c7-4bdf-bc1d-d528efbebb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, meta_model, X):\n",
    "    meta_prediction = np.zeros((X.shape[0], len(models)))\n",
    "    for i, model in enumerate(models):\n",
    "        meta_prediction[:, i] = model.predict(X)\n",
    "    return meta_model.predict(meta_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f4e359c8-c977-4cfd-a5b5-e81135be1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "base_model1 = lgbm.LGBMRegressor(\n",
    "    random_state=SEED,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=20,\n",
    "    min_data_in_leaf=12,\n",
    "    max_depth=20,\n",
    "    max_bin=160,\n",
    ")\n",
    "\n",
    "base_model2 = CatBoostRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    thread_count=2,\n",
    "    depth=7,    \n",
    "    min_data_in_leaf=10,\n",
    "    silent=True,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "\n",
    "base_model3 = BaggingRegressor(\n",
    "    n_estimators=200,\n",
    "    n_jobs=2,\n",
    "    random_state=SEED,\n",
    "    max_features=0.7,\n",
    "    max_samples=0.8,\n",
    ")\n",
    "\n",
    "base_model4 = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    criterion='mse',\n",
    "    n_jobs=2,\n",
    "    random_state=SEED,\n",
    "    max_features='log2',\n",
    "    bootstrap=False,\n",
    "    max_depth=23,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=1.6746306849395426e-07\n",
    ")\n",
    "\n",
    "base_model5 = GradientBoostingRegressor(\n",
    "    learning_rate=0.3830043942496965,\n",
    "    n_estimators=100,\n",
    "    criterion='friedman_mse',\n",
    "    max_depth=10,\n",
    "    subsample=0.9115382776453557,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "base_model6 = XGBRegressor(\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2da80965-1024-471c-9fbf-f91f11b0cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▍                                                                                             | 1/5 [00:03<00:14,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████▊                                                                      | 2/5 [00:06<00:10,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████▏                                              | 3/5 [00:11<00:07,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 4/5 [00:14<00:03,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:18<00:00,  3.62s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:41<00:00,  8.31s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:30<00:00, 18.07s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.98s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:11<00:00, 14.32s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:27<00:00,  5.52s/it]\n"
     ]
    }
   ],
   "source": [
    "model1_oof_train, model1_oof_val = get_oof(base_model1, X_train, X_val, f, \"log_price\", buildings_train_data)\n",
    "model2_oof_train, model2_oof_val = get_oof(base_model2, X_train, X_val, f, \"log_price\", buildings_train_data)\n",
    "model3_oof_train, model3_oof_val = get_oof(base_model3, X_train, X_val, f, \"log_price\", buildings_train_data)\n",
    "model4_oof_train, model4_oof_val = get_oof(base_model4, X_train, X_val, f, \"log_price\", buildings_train_data)\n",
    "model5_oof_train, model5_oof_val = get_oof(base_model5, X_train, X_val, f, \"log_price\", buildings_train_data)\n",
    "model6_oof_train, model6_oof_val = get_oof(base_model6, X_train, X_val, f, \"log_price\", buildings_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "41890a52-a48c-4da7-8924-562c565f499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_m = np.concatenate((\n",
    "    model1_oof_train,\n",
    "    model2_oof_train,\n",
    "    model3_oof_train,\n",
    "    model4_oof_train,\n",
    "    model5_oof_train,\n",
    "    model6_oof_train,\n",
    "), axis=1)\n",
    "\n",
    "x_val_m = np.concatenate((\n",
    "    model1_oof_val,\n",
    "    model2_oof_val,\n",
    "    model3_oof_val,\n",
    "    model4_oof_val,\n",
    "    model5_oof_val,\n",
    "    model6_oof_val,\n",
    "), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa72e14-ea23-4f5c-b0af-af60285d0097",
   "metadata": {},
   "source": [
    "### Training of the meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25587e-bc16-4b3f-b13b-bfbb98f7db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "META_MODEL = lgbm.LGBMRegressor(\n",
    "    random_state=SEED,\n",
    "    learning_rate=0.07,\n",
    "    n_estimators=100,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=20,\n",
    "    min_data_in_leaf=12,\n",
    "    max_depth=15,\n",
    "    max_bin=160,\n",
    ")\n",
    "\n",
    "META_MODEL.fit(x_train_m, X_train[\"log_price\"])\n",
    "train_pred = META_MODEL.predict(x_train_m)\n",
    "pred = META_MODEL.predict(x_val_m)\n",
    "\n",
    "train_rsmle = root_mean_squared_log_error(y_true=(np.e ** X_train[\"log_price\"]) - 1, y_pred=(np.e ** train_pred) - 1)\n",
    "rsmle = root_mean_squared_log_error(y_true=(np.e ** X_val[\"log_price\"]) - 1, y_pred=(np.e ** pred) - 1)\n",
    "print(\"Train rsmle: {} | Val rsmle: {}\".format(train_rsmle, rsmle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5c4d9-8657-4e49-9bf0-988fbdccff33",
   "metadata": {},
   "source": [
    "### Prediction using model 2\n",
    "The base models are retrained on the entire training set, while the meta model remains as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e00d2dfb-151e-412f-9552-a1fc9b43cb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    }
   ],
   "source": [
    "base_model1.fit(train_data[f], train_data[\"log_price\"])\n",
    "base_model2.fit(train_data[f], train_data[\"log_price\"])\n",
    "base_model3.fit(train_data[f], train_data[\"log_price\"])\n",
    "base_model4.fit(train_data[f], train_data[\"log_price\"])\n",
    "base_model5.fit(train_data[f], train_data[\"log_price\"])\n",
    "base_model6.fit(train_data[f], train_data[\"log_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50179c-cce2-42e0-8d05-fb7286afca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_pred = base_model1.predict(test_data[f])\n",
    "model2_pred = base_model2.predict(test_data[f])\n",
    "model3_pred = base_model3.predict(test_data[f])\n",
    "model4_pred = base_model4.predict(test_data[f])\n",
    "model5_pred = base_model5.predict(test_data[f])\n",
    "model6_pred = base_model6.predict(test_data[f])\n",
    "\n",
    "meta_preds = np.array([model1_pred, model2_pred, model3_pred, model4_pred, model5_pred, model6_pred])\n",
    "meta_preds = meta_preds.T\n",
    "\n",
    "pred = META_MODEL.predict(meta_preds)\n",
    "pred = (np.e ** pred) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c52da768-4028-4672-ad87-26e61fb8b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission_meta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5c85c-a355-4fba-a0e7-baf59f0f3543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
