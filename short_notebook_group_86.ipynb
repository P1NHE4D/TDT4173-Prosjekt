{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47ba4ac-adf9-4c08-bad0-68610f47dca6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Moscow Housing, Group 86\n",
    "Henrik Ytrehus Ågotnes, studentnr: 491752\n",
    "\n",
    "Alexander Michael Gerlach, studentnr: 557833\n",
    "\n",
    "Fredrik Veidahl Aagaard, studentnr: 759482\n",
    "\n",
    "**NOTE:** We manually selected two scores on kaggle. We attached the files to our submission marked as `submission_model1.csv` with a (public) score of 0.15885 corresponding to model 1 and `submission_model2.csv` with a (public) score of 0.15959 corresponding to model2. Both models are in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d9d93-afa7-4dea-a014-34f3eb0cb3c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a94300-6f87-49cc-bdec-660cbb7b4fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "import geojson\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea19ac-1770-43f6-8578-60cb3342e7c2",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbc63a8-2e3a-4b00-9b8d-ef7abd01113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments_train = pd.read_csv('data/apartments_train.csv')\n",
    "buildings_train = pd.read_csv('data/buildings_train.csv')\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "\n",
    "train = pd.merge(\n",
    "    apartments_train, \n",
    "    buildings_train.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")\n",
    "test = pd.merge(\n",
    "    apartments_test, \n",
    "    buildings_test.set_index('id'), \n",
    "    how='left', \n",
    "    left_on='building_id', \n",
    "    right_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2bdce-fbf2-40da-a78a-c58a1be54e16",
   "metadata": {},
   "source": [
    "## Replace missing coordinates and districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4dbd51-efc0-4c8d-94d5-c8ce941ea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = test[test[\"longitude\"].isna()].index\n",
    "test.at[ind, \"latitude\"] = 55.56776345324702\n",
    "test.at[ind, \"longitude\"] = 37.48171529826662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4376456d-cf66-4482-9f4a-47b925077a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = train[train.district.isna()].index\n",
    "train.at[ind, \"district\"] = 12\n",
    "ind = test[test.district.isna()].index\n",
    "test.at[ind, \"district\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baac6a5-b58e-4fe1-8ad1-0a64f44722f9",
   "metadata": {},
   "source": [
    "## Fixing incorrect coordinates in testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8927b11e-f046-4548-b769-e53c70da927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "apartment_coordinates = {\n",
    "    2511: [55.5438629741104, 37.48233890768276],\n",
    "    2529: [55.66866253043727, 37.217152651528785],\n",
    "    4719: [55.63132797843279, 37.426744466015705],\n",
    "    5090: [55.54390546580924, 37.48229599441532],\n",
    "    6959: [55.54390546580924, 37.48229599441532],\n",
    "    8596: [55.54390546580924, 37.48229599441532],\n",
    "    9547: [55.63399775464791, 37.41982879208156]\n",
    "}\n",
    "for k, v in apartment_coordinates.items():\n",
    "    test.at[k, \"latitude\"] = v[0]\n",
    "    test.at[k, \"longitude\"] = v[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a5bac-acc9-411c-b834-7ed398a569c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf37425-8a12-4473-bf3b-15124149510f",
   "metadata": {},
   "source": [
    "## Target features in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447d9a8d-bc1b-4528-bb01-06d844ce4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"log_price\"] = np.log1p(train.price)\n",
    "train[\"price_per_m2\"] = train.price / train.area_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8921ffed-b1f6-4b94-a254-d53550abef4f",
   "metadata": {},
   "source": [
    "## Feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf4a7c4-da99-4477-abf4-df3f95205cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_center_feature(data):\n",
    "    moscow_center = [55.751244, 37.618423]\n",
    "    coordinates = data[['latitude', 'longitude']].to_numpy()\n",
    "    dist = [geopy.distance.distance(moscow_center, coordinate).km for coordinate in coordinates]\n",
    "    data['distance_to_center'] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6595f4c-371c-4470-8020-89579d8236cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_metro_feature(data, add_metro_lines=True):\n",
    "    d = data.copy()\n",
    "\n",
    "    metro_data = pd.read_csv(\"data/moscow_metro_data.csv\", delimiter=\";\")\n",
    "\n",
    "    # drop duplicates\n",
    "    ind = metro_data[metro_data[\"English transcription\"].duplicated()].index\n",
    "    metro_data = metro_data.drop(ind)\n",
    "    metro_data = metro_data.reset_index()\n",
    "    metro_lines = [\"metro_line_{}\".format(i) for i in range(1, 16)]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(metro_data[[\"latitude\", \"longitude\"]])\n",
    "    distances, indices = nbrs.kneighbors(d[[\"latitude\", \"longitude\"]])\n",
    "    d[\"metro_index\"] = indices\n",
    "\n",
    "    for ind, row in d.iterrows():\n",
    "        metro_coordinate = metro_data.loc[[row.metro_index]][[\"latitude\", \"longitude\"]].to_numpy()\n",
    "        distance = geopy.distance.distance([[row[\"latitude\"], row[\"longitude\"]]], metro_coordinate).km\n",
    "        data.at[ind, \"distance_to_metro\"] = distance\n",
    "\n",
    "        if add_metro_lines:\n",
    "            number_of_metro_lines = 0\n",
    "            metro_lines_data = metro_data.loc[[row.metro_index]][metro_lines]\n",
    "            for metro_line in metro_lines:\n",
    "                access_to_line = metro_lines_data[metro_line].values[0]\n",
    "                data.at[ind, metro_line] = access_to_line\n",
    "                if access_to_line == 1:\n",
    "                    number_of_metro_lines += 1\n",
    "            data.at[ind, \"number_of_metro_lines\"] = number_of_metro_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccdd1d1-2503-4dfe-b274-6e94dc19a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearing_feature(data):\n",
    "    for ind, row in data.iterrows():\n",
    "        moscow_center = [55.751244, 37.618423]\n",
    "        c1 = moscow_center\n",
    "        c2 = [row[\"latitude\"], row[\"longitude\"]]\n",
    "        y = np.sin(c2[1] - c1[1]) * np.cos(c2[0])\n",
    "        x = np.cos(c1[0]) * np.sin(c2[0]) - np.sin(c1[0]) * np.cos(c2[0]) * np.cos(c2[1] - c1[1])\n",
    "        rad = np.arctan2(y, x)\n",
    "        bearing = ((rad * 180 / np.pi) + 360) % 360\n",
    "        data.at[ind, \"bearing\"] = bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcfe680f-98ee-480f-ad3e-7d4abfa7a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_universities_feature(data):\n",
    "    state_uni_coordinates = [55.70444300116007, 37.528611852796914]\n",
    "    tech_uni_coordinates = [55.76666597872545, 37.68511242319504]\n",
    "    distances_to_state_uni = []\n",
    "    distances_to_tech_uni = []\n",
    "    for ind, row in data.iterrows():\n",
    "        distances_to_state_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], state_uni_coordinates).km)\n",
    "        distances_to_tech_uni.append(geopy.distance.distance([row[\"latitude\"], row[\"longitude\"]], tech_uni_coordinates).km)\n",
    "    data[\"distance_to_state_uni\"] = distances_to_state_uni\n",
    "    data[\"distance_to_tech_uni\"] = distances_to_tech_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35c0b7b-313a-4d77-b3e3-be909eaabc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wealthy_districts_features(data):\n",
    "    with open(\"data/geojson/khamovniki_polygon_data.geojson\") as f:\n",
    "        gj_khamovniki = geojson.load(f)\n",
    "    with open(\"data/geojson/yakimanka_polygon_data.geojson\") as f:\n",
    "        gj_yakimanka = geojson.load(f)\n",
    "    with open(\"data/geojson/arbat_polygon_data.geojson\") as f:\n",
    "        gj_arbat = geojson.load(f)\n",
    "    with open(\"data/geojson/presnensky_polygon_data.geojson\") as f:\n",
    "        gj_presnensky = geojson.load(f)\n",
    "    with open(\"data/geojson/Tverskoy_polygon_data.geojson\") as f:\n",
    "        gj_tverskoy = geojson.load(f)\n",
    "    khamovniki_polygon = Polygon(gj_khamovniki[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    yakimanka_polygon = Polygon(gj_yakimanka[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    arbat_polygon = Polygon(gj_arbat[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    presnensky_polygon = Polygon(gj_presnensky[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    tverskoy_polygon = Polygon(gj_tverskoy[\"geometries\"][0][\"coordinates\"][0][0])\n",
    "    for ind, row in data.iterrows():\n",
    "        point = Point([row[\"longitude\"], row[\"latitude\"]])\n",
    "        data.at[ind, \"is_in_khamovniki\"] = 1 if khamovniki_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_yakimanka\"] = 1 if yakimanka_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_arbat\"] = 1 if arbat_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_presnensky\"] = 1 if presnensky_polygon.contains(point) else 0\n",
    "        data.at[ind, \"is_in_tverskoy\"] = 1 if tverskoy_polygon.contains(point) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e392b68d-0753-417b-8193-4a3272c498a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_box_areas_more_boxes(data):\n",
    "    \"\"\"only add values in list to prevent unnecessary slowdown\"\"\"\n",
    "    locs_with_high_corr = [\n",
    "            'loc_37.575_55.75', 'loc_37.6_55.725', 'loc_37.525_55.75',\n",
    "           'loc_37.55_55.775', 'loc_37.525_55.725', 'loc_37.575_55.725',\n",
    "           'loc_37.525_55.7', 'loc_37.5_55.7', 'loc_37.55_55.725',\n",
    "           'loc_37.475_55.7', 'loc_37.625_55.725', 'loc_37.5_55.75',\n",
    "           'loc_37.525_55.775', 'loc_37.525_55.675', 'loc_37.55_55.675',\n",
    "           'loc_37.55_55.75', 'loc_37.4_55.6', 'loc_37.725_55.575',\n",
    "           'loc_37.925_55.675', 'loc_37.475_55.55', 'loc_37.65_55.575',\n",
    "           'loc_37.525_55.875', 'loc_37.5_55.55', 'loc_37.475_55.525',\n",
    "           'loc_37.925_55.7'\n",
    "    ]\n",
    "    for lon in np.arange(37.4, 37.95, 0.025):\n",
    "        for lat in np.arange(55.525, 55.9, 0.025):\n",
    "            column_name = \"loc_\" + str(round(lon, 3)) + \"_\" + str(round(lat, 3))\n",
    "            if column_name in locs_with_high_corr: #column_name in locs_with_high_corr:\n",
    "                data[column_name] = 0\n",
    "                indexes = data[\n",
    "                    (lon < data['longitude']) & (data['longitude'] < lon + 0.025) & (lat < data['latitude']) & (\n",
    "                                data['latitude'] < lat + 0.025)\n",
    "                    ].index\n",
    "\n",
    "                data.loc[indexes, column_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c863633-7977-44db-92f0-5f3d8fddbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def add_clusters(train_data, test_data):\n",
    "    kmeans = KMeans(n_clusters=100, random_state=0)\n",
    "    s1 = train_data.longitude\n",
    "    s2 = test_data.longitude\n",
    "    f1 = train_data.latitude\n",
    "    f2 = test_data.latitude\n",
    "    kmeans.fit(pd.concat([pd.concat([s1, s2]), pd.concat([f1, f2]) ], axis=1))\n",
    "    train_data['clusters'] = kmeans.predict(train_data[['longitude', 'latitude']])\n",
    "    pd.get_dummies(train_data, columns=['clusters'])\n",
    "    test_data['clusters'] = kmeans.predict(test_data[['longitude', 'latitude']])\n",
    "    test_data = pd.get_dummies(test_data,columns=['clusters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39934d84-67ee-45ab-aef4-30c8b37872c9",
   "metadata": {},
   "source": [
    "## Feature construction - (NOTE: TAKES SOME TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e2cd22-1411-40a8-b927-db928bdc94ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features(data):\n",
    "    distance_to_center_feature(data)\n",
    "    distance_to_metro_feature(data, False)\n",
    "    bearing_feature(data)\n",
    "    distance_to_universities_feature(data)\n",
    "    #wealthy_districts_features(data)\n",
    "    add_box_areas_more_boxes(data)\n",
    "\n",
    "train_with_feats = train.copy()\n",
    "test_with_feats = test.copy()\n",
    "construct_features(train_with_feats)\n",
    "construct_features(test_with_feats)\n",
    "add_clusters(train_with_feats, test_with_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396131b3-6548-4874-996d-3e998e697caa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f158e7-3087-4006-aad6-bf3a80292cef",
   "metadata": {},
   "source": [
    "## Imputing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d840e1a7-180f-453d-be1c-3b06b6745a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_bin_mean(data, train_data, test_data, feature, bin_feature, bins=40, decimals=0, verbose=False):\n",
    "    tr = train_data.copy()\n",
    "    te = test_data.copy()\n",
    "    database = pd.concat([tr, te])\n",
    "    database[\"bin\"], bins = pd.cut(database[bin_feature], bins=40, retbins=True)\n",
    "    pb = 0\n",
    "    for b in bins:\n",
    "        bin_data = database[(database[bin_feature] >= pb) & (database[bin_feature] <= b)]\n",
    "        bin_mean = bin_data[feature].mean()\n",
    "        percentage = (bin_mean / bin_data[bin_feature]).mean()\n",
    "        d = data[(data[bin_feature] >= pb) & (data[bin_feature] <= b)]\n",
    "        ind = d[d[feature].isna()].index\n",
    "        if pd.notna(bin_mean) and len(ind) > 0:\n",
    "            bin_mean = round(bin_mean, decimals)\n",
    "            data.at[ind, feature] = bin_mean\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Set {} for {} rows with {} <= {} <= {} to {}\".format(feature, len(ind), pb, bin_feature, b, bin_mean))\n",
    "        pb = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e38993fb-c97d-4b46-b88c-4b7e94895c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_categorical_features(data):\n",
    "    data.loc[:, \"seller\"] = data.loc[:, \"seller\"].fillna(4)\n",
    "    data.loc[:, \"layout\"] = data.loc[:, \"layout\"].fillna(3)\n",
    "    data.loc[:, \"condition\"] = data.loc[:, \"condition\"].fillna(4)\n",
    "    data.loc[:, \"material\"] = data.loc[:, \"material\"].fillna(7)\n",
    "    data.loc[:, \"parking\"] = data.loc[:, \"parking\"].fillna(3)\n",
    "    data.loc[:, \"heating\"] = data.loc[:, \"heating\"].fillna(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc84021e-7102-4d3c-b011-4f00874053cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_boolean_features(data):\n",
    "    data.loc[:, \"elevator_without\"] = data.loc[:, \"elevator_without\"].fillna(0)\n",
    "    data.loc[:, \"elevator_service\"] = data.loc[:, \"elevator_service\"].fillna(0)\n",
    "    data.loc[:, \"elevator_passenger\"] = data.loc[:, \"elevator_passenger\"].fillna(0)\n",
    "    data.loc[:, \"garbage_chute\"] = data.loc[:, \"garbage_chute\"].fillna(0)\n",
    "    data.loc[:, \"balconies\"] = data.loc[:, \"balconies\"].fillna(0)\n",
    "    data.loc[:, \"loggias\"] = data.loc[:, \"loggias\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d033824-a750-4547-8e54-e55ccfe9099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_numerical_features(data):\n",
    "    # constructed\n",
    "    data.loc[data.new == 1, \"constructed\"] = data.loc[data.new == 1, \"constructed\"].fillna(2021)\n",
    "    impute_with_bin_mean(data=data, train_data=train, test_data=test, feature=\"constructed\", bins=12, bin_feature=\"district\")\n",
    "    data.loc[data.constructed >= 2021, \"new\"] = data.loc[data.constructed >= 2021, \"new\"].fillna(1)\n",
    "    data.loc[data.constructed < 2021, \"new\"] = data.loc[data.constructed < 2021, \"new\"].fillna(0)\n",
    "    \n",
    "    numerical_features = [\"phones\", \"ceiling\", \"bathrooms_shared\", \"bathrooms_private\", \"windows_street\", \"windows_court\", \"area_kitchen\", \"area_living\"]\n",
    "    for num_feat in numerical_features:\n",
    "        impute_with_bin_mean(data=data, train_data=train, test_data=test, feature=num_feat, bin_feature=\"area_total\", bins=20)\n",
    "        fill_value = data[data[num_feat].notna()][num_feat].mean(axis=0).round()\n",
    "        data.loc[:, num_feat] = data.loc[:, num_feat].fillna(fill_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "936f4e0f-202b-4903-b423-c6d728dd72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nans(data):\n",
    "    impute_categorical_features(data)\n",
    "    impute_boolean_features(data)\n",
    "    impute_numerical_features(data)\n",
    "\n",
    "train_data = train_with_feats.copy()\n",
    "test_data = test_with_feats.copy()\n",
    "impute_nans(train_data)\n",
    "impute_nans(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223be9d1-7c75-4db7-8861-07ac31f3b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data.area_living==0, 'area_living'] = train_data[train_data.area_living==0].area_total * 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a8d72-3269-4743-817d-f1b10b01027b",
   "metadata": {},
   "source": [
    "## One-hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518f4b13-c4cf-47ac-b9ad-619b77375991",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_categories = {\n",
    "    3: \"East\",\n",
    "    6: \"South-West\",\n",
    "    5: \"South\",\n",
    "    4: \"South-East\",\n",
    "    0: \"Central\",\n",
    "    2: \"North-East\",\n",
    "    1: \"North\",\n",
    "    8: \"North-West\",\n",
    "    7: \"West\",\n",
    "    11: \"Novomoskovsk\",\n",
    "    10: \"Troitsk\",\n",
    "    9: \"Zelenograd\",\n",
    "    12: \"Other_district\"\n",
    "}\n",
    "material_categories = {\n",
    "    0: \"Bricks\",\n",
    "    1: \"Wood\",\n",
    "    2: \"Monolith\",\n",
    "    3: \"Panel\",\n",
    "    4: \"Block\",\n",
    "    5: \"Monolithic_brick\",\n",
    "    6: \"Stalin_project\",\n",
    "    7: \"Other_material\"\n",
    "}\n",
    "heating_categories = {\n",
    "    0: \"Central\",\n",
    "    1: \"Individual\",\n",
    "    2: \"Boiler\",\n",
    "    3: \"Autonomous_boiler\",\n",
    "    4: \"Other_heating\"\n",
    "}\n",
    "parking_categories = {\n",
    "    0: \"Ground\",\n",
    "    1: \"Underground\",\n",
    "    2: \"Multilevel\",\n",
    "    3: \"No_parking\"\n",
    "}\n",
    "seller_categories = {\n",
    "    0: \"Owner\",\n",
    "    1: \"Company\",\n",
    "    2: \"Agents\",\n",
    "    3: \"Developer\",\n",
    "    4: \"Other_seller\"\n",
    "}\n",
    "layout_categoires = {\n",
    "    0: \"Adjacent\",\n",
    "    1: \"Isolated\",\n",
    "    2: \"Adjacent_isolated\",\n",
    "    3: \"Other_layout\"\n",
    "}\n",
    "condition_categories = {\n",
    "    0: \"Undecorated\",\n",
    "    1: \"Decorated\",\n",
    "    2: \"Euro_repair\",\n",
    "    3: \"Special_design\",\n",
    "    4: \"Other_condition\"\n",
    "}\n",
    "categorical_features = {\n",
    "    \"parking\": parking_categories,\n",
    "    \"heating\": heating_categories,\n",
    "    \"material\": material_categories,\n",
    "    \"district\": district_categories,\n",
    "    \"seller\": seller_categories,\n",
    "    \"layout\": layout_categoires,\n",
    "    \"condition\": condition_categories\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5119f5-76df-4086-99a3-8db469089220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_categorical_features(data, remove_org=True):\n",
    "    for orig_feat, categories in categorical_features.items():\n",
    "        for k, category in categories.items():\n",
    "            data[category] = (data[orig_feat] == k).astype(int)\n",
    "        if remove_org:\n",
    "            data.drop(orig_feat, axis=1, inplace=True)\n",
    "one_hot_encode_categorical_features(train_data)\n",
    "one_hot_encode_categorical_features(test_data)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['clusters'])\n",
    "test_data = pd.get_dummies(test_data, columns=['clusters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755673bc-b62b-4b7d-999c-d4b5f031ec1b",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f93025-a0c8-4de4-adb2-a66e30a29572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for model 1\n",
    "features = train_data.columns\n",
    "features = features.drop([\"id\", \"building_id\", \"street\", \"address\", \"price\", \"log_price\", \"price_per_m2\", \"street\", \"address\"], errors=\"ignore\")\n",
    "features = features.drop([col for col in train_data.columns if col[0:2] == 'cl']) \n",
    "\n",
    "# features for model 2\n",
    "features_2 = train_data.columns\n",
    "features_2 = features.drop([\"id\", \"building_id\", \"street\", \"address\", \"price\", \"log_price\", \"price_per_m2\", \"street\", \"address\"], errors=\"ignore\")\n",
    "features_2 = features.drop([col for col in train_data.columns if col[0:3] == 'loc']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289eb44-f2a5-427d-ad18-c12f97a62bd3",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "342726bb-08c1-4eb2-b63a-d50a1907ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples before: 23285 | Samples after: 23062\n"
     ]
    }
   ],
   "source": [
    "def remove_min_max_outliers(train_data, test_data, features):\n",
    "    t = train_data.copy()\n",
    "    test_feat_min_max = pd.DataFrame({\"max\": test_data.describe().loc[\"max\"], \"min\": test_data.describe().loc[\"min\"]}).T\n",
    "    for feature in features:\n",
    "        max_val = test_feat_min_max[feature][\"max\"]\n",
    "        min_val = test_feat_min_max[feature][\"min\"]\n",
    "        max_ind = t[t[feature] > max_val].index\n",
    "        min_ind = t[t[feature] < min_val].index\n",
    "        t.drop(max_ind, inplace=True)\n",
    "        t.drop(min_ind, inplace=True)\n",
    "\n",
    "    print(\"Samples before: {} | Samples after: {}\".format(len(train_data), len(t)))\n",
    "    return t\n",
    "\n",
    "train_data = remove_min_max_outliers(train_data, test_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f23b07bb-35a6-4fe9-8b29-caa7831e8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6096f2-f09c-456a-8eda-ff1ed3ee56b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model definition, training, and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ecc2c5e-18fe-4aef-a5b9-5b32af0d21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206af387-4854-4868-b099-85837da367ee",
   "metadata": {},
   "source": [
    "## Model 1 - using stacking with averaging and box_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "075ab44d-8dac-47f3-9114-bb8346aea152",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "model1 = lgbm.LGBMRegressor(\n",
    "    random_state=SEED,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=2000,\n",
    "    boosting_type='gbdt',\n",
    "    n_jobs=2,\n",
    "    num_leaves=20,\n",
    "    min_data_in_leaf=12,\n",
    "    max_depth=20,\n",
    "    max_bin=160,\n",
    ")\n",
    "\n",
    "model2 = CatBoostRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.2,\n",
    "    thread_count=2,\n",
    "    depth=7,    \n",
    "    min_data_in_leaf=10,\n",
    "    silent=True,\n",
    "    random_seed=SEED,\n",
    ")\n",
    "\n",
    "model3 = BaggingRegressor(\n",
    "    n_estimators=200,\n",
    "    n_jobs=2,\n",
    "    random_state=SEED,\n",
    "    max_features=0.7,\n",
    "    max_samples=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e2638b-a632-462d-b620-e47d6d2e9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(model, k, data, features, target_feature):\n",
    "    kf = KFold(\n",
    "        n_splits=k,\n",
    "        shuffle=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for i, (train_index, test_index) in tqdm(enumerate(kf.split(buildings_train)), total=k):\n",
    "        building_val_ids = buildings_train.id.values[test_index]\n",
    "        train = data[~data.building_id.isin(building_val_ids)]\n",
    "        val = data[data.building_id.isin(building_val_ids)]\n",
    "        X_train = train[features].values\n",
    "        X_val = val[features].values\n",
    "        y_train = train[[target_feature]].values.ravel()\n",
    "        y_val = val[[target_feature]].values.ravel()\n",
    "        y_train_true = train[[\"price\"]].values.ravel()\n",
    "        y_val_true = val[[\"price\"]].values.ravel()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        train_pred = model.predict(X_train)\n",
    "        val_pred = model.predict(X_val)\n",
    "        \n",
    "        if target_feature == \"log_price\":\n",
    "            train_pred = (np.e ** train_pred) - 1\n",
    "            val_pred = (np.e ** val_pred) - 1\n",
    "        elif target_feature == \"price_per_m2\":\n",
    "            train_pred = train_pred * train.area_total\n",
    "            val_pred = val_pred * val.area_total\n",
    "        else:\n",
    "            raise Exception(\"Unknown target feature\")\n",
    "            \n",
    "        train_rsmle = root_mean_squared_log_error(y_true=y_train_true, y_pred=train_pred)\n",
    "        val_rsmle = root_mean_squared_log_error(y_true=y_val_true, y_pred=val_pred)\n",
    "        train_scores.append(train_rsmle)\n",
    "        val_scores.append(val_rsmle)\n",
    "    train_scores = np.array(train_scores)\n",
    "    val_scores = np.array(val_scores)\n",
    "    print(\"Train rsmle: {} | Std: {}\".format(train_scores.mean(), train_scores.std()))\n",
    "    print(\"Val rsmle: {} | Std: {}\".format(val_scores.mean(), val_scores.std()))\n",
    "    return val_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca9ffac9-ba6f-4375-958e-b967623536f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▍                                                                                             | 1/5 [00:05<00:22,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████▊                                                                      | 2/5 [00:11<00:16,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████▏                                              | 3/5 [00:16<00:10,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 4/5 [00:21<00:05,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:29<00:00,  5.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.04201944015599936 | Std: 0.0007927884079163581\n",
      "Val rsmle: 0.20152252789785347 | Std: 0.024818056998283763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:34<00:00,  6.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.05191085408711893 | Std: 0.0007495411745402716\n",
      "Val rsmle: 0.20930695670556912 | Std: 0.033408242387527536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:07<00:00, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.0627379174016011 | Std: 0.0011539479574389216\n",
      "Val rsmle: 0.2093521017162976 | Std: 0.03191689266337697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1_scores = cross_validation(model1, 5, train_data, features, \"log_price\")\n",
    "model2_scores = cross_validation(model2, 5, train_data, features, \"log_price\")\n",
    "model3_scores = cross_validation(model3, 5, train_data, features, \"price_per_m2\")\n",
    "scores = {\n",
    "    \"lgbm\": model1_scores,\n",
    "    \"cb\": model2_scores,\n",
    "    \"bagging\": model3_scores,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1032472-c666-4b1e-9135-5d22bd69e884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsmle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.201523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>0.209307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging</th>\n",
       "      <td>0.209352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rsmle\n",
       "lgbm     0.201523\n",
       "cb       0.209307\n",
       "bagging  0.209352"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame(scores, index=[0])\n",
    "model_scores = model_scores.T\n",
    "model_scores.columns = [\"rsmle\"]\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70d1f2-73f6-4567-bbd2-d44b3f283c38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction using model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "612733ee-b17a-4c1a-88e4-f90e9ae8b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_data[features], train_data[\"log_price\"])\n",
    "model2.fit(train_data[features], train_data[\"log_price\"])\n",
    "model3.fit(train_data[features], train_data[\"price_per_m2\"])\n",
    "\n",
    "model1_pred = model1.predict(test_data[features])\n",
    "model2_pred = model2.predict(test_data[features])\n",
    "model3_pred = model3.predict(test_data[features])\n",
    "\n",
    "model1_pred = (np.e ** model1_pred) - 1\n",
    "model2_pred = (np.e ** model2_pred) - 1\n",
    "model3_pred *= test_data.area_total\n",
    "\n",
    "predictions = [model1_pred, model2_pred, model3_pred]\n",
    "pred = np.average(predictions, \n",
    "    weights = 1 / model_scores['rsmle'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission_model1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c3372-9a33-44e8-9634-866b533a0485",
   "metadata": {},
   "source": [
    "## Model 2 - stacking with averaging and clusters instead of box_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3197c3a-cec4-4fa1-9911-fd0890d751f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████▍                                                                                             | 1/5 [00:05<00:20,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████████████▊                                                                      | 2/5 [00:10<00:15,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████▏                                              | 3/5 [00:15<00:09,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████▌                       | 4/5 [00:19<00:04,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:22<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.04154846719540952 | Std: 0.0006239318570326423\n",
      "Val rsmle: 0.19928084042973432 | Std: 0.020495137796523448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:36<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.051509534830374834 | Std: 0.0007875993107409987\n",
      "Val rsmle: 0.20463721754870146 | Std: 0.02078200122532646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:01<00:00, 24.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rsmle: 0.06279062923730641 | Std: 0.0011365991375978698\n",
      "Val rsmle: 0.2092565787340182 | Std: 0.032681001329564544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1_scores = cross_validation(model1, 5, train_data, features_2, \"log_price\")\n",
    "model2_scores = cross_validation(model2, 5, train_data, features_2, \"log_price\")\n",
    "model3_scores = cross_validation(model3, 5, train_data, features_2, \"price_per_m2\")\n",
    "scores = {\n",
    "    \"lgbm\": model1_scores,\n",
    "    \"cb\": model2_scores,\n",
    "    \"bagging\": model3_scores,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "084ea0be-4199-40d7-b55e-4d6f391e47b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rsmle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgbm</th>\n",
       "      <td>0.199281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>0.204637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging</th>\n",
       "      <td>0.209257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rsmle\n",
       "lgbm     0.199281\n",
       "cb       0.204637\n",
       "bagging  0.209257"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores = pd.DataFrame(scores, index=[0])\n",
    "model_scores = model_scores.T\n",
    "model_scores.columns = [\"rsmle\"]\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e369ba38-7c57-45d2-8a15-5777c4fc0180",
   "metadata": {},
   "source": [
    "## Prediction using model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8731a3d6-7a5f-49be-bb86-a9c438f011e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=12\n"
     ]
    }
   ],
   "source": [
    "model1.fit(train_data[features_2], train_data[\"log_price\"])\n",
    "model2.fit(train_data[features_2], train_data[\"log_price\"])\n",
    "model3.fit(train_data[features_2], train_data[\"price_per_m2\"])\n",
    "\n",
    "model1_pred = model1.predict(test_data[features_2])\n",
    "model2_pred = model2.predict(test_data[features_2])\n",
    "model3_pred = model3.predict(test_data[features_2])\n",
    "\n",
    "model1_pred = (np.e ** model1_pred) - 1\n",
    "model2_pred = (np.e ** model2_pred) - 1\n",
    "model3_pred *= test_data.area_total\n",
    "\n",
    "predictions = [model1_pred, model2_pred, model3_pred]\n",
    "pred = np.average(predictions, \n",
    "    weights = 1 / model_scores['rsmle'] ** 4,\n",
    "    axis=0\n",
    ")\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data.id\n",
    "submission['price_prediction'] = pred\n",
    "submission.to_csv('submission_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bb811-7f25-4161-aaba-3131d156d2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
